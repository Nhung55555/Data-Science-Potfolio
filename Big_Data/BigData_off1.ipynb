{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvITKeEzSb4x"
      },
      "source": [
        "# import data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hX49Mwlo-kc"
      },
      "source": [
        "#Using apache spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RVQkOJ6cpJe3",
        "outputId": "165441ba-9827-49ff-a2d2-5b8c70da5544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: findspark in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: implicit in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.7.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (2.2.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from implicit) (3.6.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.10.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (2.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (2.2.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn) (3.10.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: jupyter in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
            "Requirement already satisfied: ipykernel in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter) (6.29.5)\n",
            "Requirement already satisfied: jupyter-console in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (6.6.3)\n",
            "Requirement already satisfied: jupyterlab in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (4.4.2)\n",
            "Requirement already satisfied: ipywidgets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (8.1.7)\n",
            "Requirement already satisfied: notebook in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.4.2)\n",
            "Requirement already satisfied: nbconvert in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.16.6)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (5.14.3)\n",
            "Requirement already satisfied: psutil in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (7.0.0)\n",
            "Requirement already satisfied: appnope in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.1.4)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (5.7.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (6.4.2)\n",
            "Requirement already satisfied: comm>=0.1.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.2.2)\n",
            "Requirement already satisfied: packaging in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (25.0)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.8.14)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (8.6.3)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (8.36.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (26.4.0)\n",
            "Requirement already satisfied: nest-asyncio in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.1.7)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets->jupyter) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets->jupyter) (3.0.15)\n",
            "Requirement already satisfied: pygments in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-console->jupyter) (2.19.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-console->jupyter) (3.0.51)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.16.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.0.5)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.2.1)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (65.5.0)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (3.1.3)\n",
            "Requirement already satisfied: bleach[css]!=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (6.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.10.2)\n",
            "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.1)\n",
            "Requirement already satisfied: nbformat>=5.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (5.10.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (4.13.4)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from async-lru>=1.0.0->jupyterlab->jupyter) (4.13.2)\n",
            "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
            "Requirement already satisfied: stack_data in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
            "Requirement already satisfied: exceptiongroup in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
            "Requirement already satisfied: decorator in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.7)\n",
            "Requirement already satisfied: jupyter-events>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.22.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
            "Requirement already satisfied: babel>=2.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
            "Requirement already satisfied: requests>=2.31 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
            "Requirement already satisfied: wcwidth in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
            "Requirement already satisfied: pure-eval in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/giahuy/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
            "Requirement already satisfied: isoduration in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (20.11.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.11.1)\n",
            "Requirement already satisfied: uri-template in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.0.0)\n",
            "Requirement already satisfied: fqdn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.5.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.9.0.20250516)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: wget in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: unzip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Download Spark\n",
        "%pip install pyspark\n",
        "%pip install findspark\n",
        "\n",
        "#Install implicit\n",
        "%pip install implicit\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install os\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install jupyter\n",
        "%pip install wget\n",
        "%pip install unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4okrjU1VpazO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vXOC73WHpa7G",
        "outputId": "3bc76ea7-1d90-4b40-ce4b-5890689c27ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 11:09:02 WARN Utils: Your hostname, GIAs-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 10.236.56.117 instead (on interface en0)\n",
            "25/05/27 11:09:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/27 11:09:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 59947)\n",
            "Traceback (most recent call last):\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 347, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socketserver.py\", line 747, in __init__\n",
            "    self.handle()\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Initialize findspark and create SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookRecommendation\") \\\n",
        "    .config(\"spark.driver.memory\", \"6g\") \\\n",
        "    .config(\"spark.executor.memory\", \"6g\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
        "    .config(\"spark.driver.host\", \"localhost\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.default.parallelism\", \"200\") \\\n",
        "    .config(\"spark.network.timeout\", \"600s\") \\\n",
        "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
        "    .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHN2nBGayqHd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count, col, when, mean, rand  # Import all at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BRcHAN5Fd0NI",
        "outputId": "4037c23f-73d8-476a-e833-04f233a221be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data files already exist\n"
          ]
        }
      ],
      "source": [
        "#Check if we have the data\n",
        "# Check if data files exist\n",
        "if os.path.exists('Users.csv') and os.path.exists('Books.csv') and os.path.exists('Ratings.csv'):\n",
        "    print(\"Data files already exist\")\n",
        "    exit\n",
        "else:\n",
        "    # Import data from GitHub using wget\n",
        "    !wget -O data.zip 'https://github.com/giahuy1310/bookrecommendation/raw/main/BookDataset.zip'\n",
        "    !unzip data.zip\n",
        "    # Load data into Spark DataFrame\n",
        "users = spark.read.csv('Users.csv', header=True, inferSchema=True)\n",
        "books = spark.read.csv('Books.csv', header = True, inferSchema=True)\n",
        "ratings = spark.read.csv('Ratings.csv', header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflfS0K4pbN1",
        "outputId": "68022036-0013-4b80-d023-2975530e28eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: string (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Rating: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show schemas\n",
        "books.printSchema()\n",
        "ratings.printSchema()\n",
        "users.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxM2D5Y_BkAO",
        "outputId": "a65cbd34-3fe2-4cae-a52a-8cd5e2772cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: integer (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt:  change the type of the Age from string to float\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "users = users.withColumn(\"Age\", users[\"Age\"].cast(IntegerType()))\n",
        "users.printSchema()\n",
        "\n",
        "books = books.withColumn(\"Year-Of-Publication\", books[\"Year-Of-Publication\"].cast(IntegerType()))\n",
        "books.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXM8Dc9jBkAO"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFxMyKuCBkAP",
        "outputId": "bd8fc201-436e-481f-c2d6-644a9159c1fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 11:09:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|summary|                ISBN|          Book-Title|         Book-Author|Year-Of-Publication|           Publisher|         Image-URL-S|         Image-URL-M|         Image-URL-L|\n",
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  count|              271360|              271360|              271359|             271303|              271358|              271360|              271360|              271357|\n",
            "|   mean|1.0412234356977516E9|            Infinity|              2001.0|  1959.754171535147|  3765.6153846153848|             1992.25|              1984.5|             1988.25|\n",
            "| stddev|1.4877142833957229E9|                 NaN|  1.7320508075688772| 258.01944980678707|  7963.0591536808415|   9.863845829363632|  0.7071067811865476|    7.13559154286923|\n",
            "|    min|          0000913154| A Light in the S...| 15th Illinois Vo...|                  0| As Taught at the...| 1865. (Collector...|          Bill Stern|       Our Miss b\"\"\"|\n",
            "|    max|          B0002K6K8O|   Ã?Â?thique en toc|      Ã?Â?ric Holder|               2050|      Ã?Â¶bv&amp;hpt|http://images.ama...|http://images.ama...|http://images.ama...|\n",
            "+-------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|summary|          User-ID|          Location|               Age|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|  count|           278858|            278859|            167674|\n",
            "|   mean|         139429.5|              NULL|34.751833915812824|\n",
            "| stddev|80499.51502027822|              NULL|14.431010268447515|\n",
            "|    min|                1|             milan|                 0|\n",
            "|    max|           278858|ýzmýr, n/a, turkey|               244|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "\n",
            "+-------+------------------+-----------+------------------+\n",
            "|summary|           User-ID|       ISBN|       Book-Rating|\n",
            "+-------+------------------+-----------+------------------+\n",
            "|  count|           1149780|    1149780|           1149780|\n",
            "|   mean|140386.39512602412|   Infinity|2.8669501991685364|\n",
            "| stddev| 80562.27771851169|        NaN| 3.854183859201664|\n",
            "|    min|                 2| 0330299891|                 0|\n",
            "|    max|            278854|  Ô½crosoft|                10|\n",
            "+-------+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "books.describe().show()\n",
        "users.describe().show()\n",
        "ratings.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dXWOPgmBkAP",
        "outputId": "1d2b7059-4af2-4bea-f238-95ade6b068ad"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed Age Statistics:\n",
            "Total records: 278859\n",
            "NULL ages: 111185\n",
            "Non-NULL ages: 167674\n",
            "Percentage of NULL ages: 39.87%\n",
            "\n",
            "Age Statistics (excluding NULLs):\n",
            "+-------+------------------+\n",
            "|summary|               Age|\n",
            "+-------+------------------+\n",
            "|  count|            167674|\n",
            "|   mean|34.751833915812824|\n",
            "| stddev|14.431010268447515|\n",
            "|    min|                 0|\n",
            "|    max|               244|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a figure with two subplots\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "\n",
        "# Plot 1: Age distribution (excluding NULLs)\n",
        "age_data = users.select(\"Age\").toPandas()\n",
        "age_data = age_data.dropna()\n",
        "age_data = age_data[age_data[\"Age\"] > 0]\n",
        "ax1.hist(age_data[\"Age\"], bins=30, edgecolor='black')\n",
        "ax1.set_title('Distribution of User Ages (Excluding NULLs)')\n",
        "ax1.set_xlabel('Age')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: NULL vs Non-NULL distribution\n",
        "null_count = users.filter(col(\"Age\").isNull()).count()\n",
        "non_null_count = users.filter(col(\"Age\").isNotNull()).count()\n",
        "ax2.bar(['NULL', 'Non-NULL'], [null_count, non_null_count])\n",
        "ax2.set_title('NULL vs Non-NULL Ages')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "total_count = users.count()\n",
        "# Print detailed statistics\n",
        "print(\"\\nDetailed Age Statistics:\")\n",
        "print(f\"Total records: {users.count()}\")\n",
        "print(f\"NULL ages: {null_count}\")\n",
        "print(f\"Non-NULL ages: {non_null_count}\")\n",
        "print(f\"Percentage of NULL ages: {(null_count/total_count)*100:.2f}%\")\n",
        "\n",
        "# Show age statistics for non-NULL values\n",
        "print(\"\\nAge Statistics (excluding NULLs):\")\n",
        "users.filter(col(\"Age\").isNotNull()).select(\"Age\").describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbErI4ucBkAP",
        "outputId": "b22664d8-7886-4aab-8ced-e361c0ac43fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users before filtering:\n",
            "278859\n",
            "Number of users after filtering:\n",
            "277205\n",
            "Percentage of users filtered out:\n",
            "0.59%\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Filter users between 10 and 80 years old\n",
        "print(\"Number of users before filtering:\")\n",
        "print(users.select(\"User-ID\").distinct().count())\n",
        "filtered_users = users.filter((users.Age.isNotNull() & (users.Age >= 10) & (users.Age <= 80)) | users.Age.isNull())\n",
        "\n",
        "print(\"Number of users after filtering:\")\n",
        "print(filtered_users.select(\"User-ID\").distinct().count())\n",
        "\n",
        "print(\"Percentage of users filtered out:\")\n",
        "print(f\"{(total_count - filtered_users.select('User-ID').distinct().count()) / total_count * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4Efw29BBkAP",
        "outputId": "2b11f59e-8230-486a-e0ea-46a75667b089"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count ratings\n",
        "zero_ratings = ratings.filter(ratings['Book-Rating'] == 0).count()\n",
        "nonzero_ratings = ratings.filter(ratings['Book-Rating'] > 0).count()\n",
        "\n",
        "# Create data for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rating_counts = [zero_ratings, nonzero_ratings]\n",
        "labels = ['Rating = 0', 'Rating 1-10']\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(labels, rating_counts)\n",
        "plt.title('Distribution of Zero vs Non-Zero Ratings')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add value labels on top of each bar\n",
        "for i, v in enumerate(rating_counts):\n",
        "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuXhkvEQBkAP",
        "outputId": "5f1748c9-08f7-49a5-e6f8-3480fbe6b4fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in users DataFrame:\n",
            "+-------+--------+------+\n",
            "|User-ID|Location|   Age|\n",
            "+-------+--------+------+\n",
            "|      1|       0|111185|\n",
            "+-------+--------+------+\n",
            "\n",
            "\n",
            "Null values in ratings DataFrame:\n",
            "+-------+----+-----------+\n",
            "|User-ID|ISBN|Book-Rating|\n",
            "+-------+----+-----------+\n",
            "|      0|   0|          0|\n",
            "+-------+----+-----------+\n",
            "\n",
            "\n",
            "Null values in books DataFrame:\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|Image-URL-S|Image-URL-M|Image-URL-L|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|   0|         0|          1|                 57|        2|          0|          0|          3|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in users DataFrame\n",
        "print(\"Null values in users DataFrame:\")\n",
        "users.select([count(when(col(c).isNull(), c)).alias(c) for c in users.columns]).show()\n",
        "\n",
        "# Check for null values in ratings DataFrame\n",
        "print(\"\\nNull values in ratings DataFrame:\")\n",
        "ratings.select([count(when(col(c).isNull(), c)).alias(c) for c in ratings.columns]).show()\n",
        "\n",
        "# Check for null values in books DataFrame\n",
        "print(\"\\nNull values in books DataFrame:\")\n",
        "books.select([count(when(col(c).isNull(), c)).alias(c) for c in books.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NttG486YBkAP",
        "outputId": "ebd676f4-4d7e-4f73-eaa7-9f1fd9659782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After dropping nulls:\n",
            "\n",
            "Users count: 278858\n",
            "Ratings count: 1149780\n",
            "Books count: 271300\n"
          ]
        }
      ],
      "source": [
        "# Drop rows with null User-ID\n",
        "users = users.dropna(subset=['User-ID'])\n",
        "ratings = ratings.dropna(subset=['User-ID'])\n",
        "\n",
        "# Drop null values in books DataFrame\n",
        "books = books.dropna()\n",
        "\n",
        "print(\"After dropping nulls:\")\n",
        "print(\"\\nUsers count:\", users.count())\n",
        "print(\"Ratings count:\", ratings.count())\n",
        "print(\"Books count:\", books.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zieaFREcBkAP",
        "outputId": "006b7ad2-5af3-434b-85e4-f0c403bf33dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in users DataFrame after processing:\n",
            "+-------+--------+------+\n",
            "|User-ID|Location|   Age|\n",
            "+-------+--------+------+\n",
            "|      0|       0|111184|\n",
            "+-------+--------+------+\n",
            "\n",
            "\n",
            "Null values in ratings DataFrame after processing:\n",
            "+-------+----+-----------+\n",
            "|User-ID|ISBN|Book-Rating|\n",
            "+-------+----+-----------+\n",
            "|      0|   0|          0|\n",
            "+-------+----+-----------+\n",
            "\n",
            "\n",
            "Null values in books DataFrame after processing:\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|Image-URL-S|Image-URL-M|Image-URL-L|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "|   0|         0|          0|                  0|        0|          0|          0|          0|\n",
            "+----+----------+-----------+-------------------+---------+-----------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in users DataFrame after processing\n",
        "print(\"Null values in users DataFrame after processing:\")\n",
        "users.select([count(when(col(c).isNull(), c)).alias(c) for c in users.columns]).show()\n",
        "\n",
        "# Check for null values in ratings DataFrame after processing\n",
        "print(\"\\nNull values in ratings DataFrame after processing:\")\n",
        "ratings.select([count(when(col(c).isNull(), c)).alias(c) for c in ratings.columns]).show()\n",
        "\n",
        "# Check for null values in books DataFrame after processing\n",
        "print(\"\\nNull values in books DataFrame after processing:\")\n",
        "books.select([count(when(col(c).isNull(), c)).alias(c) for c in books.columns]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ8cWeqFBkAP",
        "outputId": "a41ff01b-e47e-4e51-b43e-e1cb8a414a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users: 277205\n",
            "Number of distinct users in ratings: 105283\n"
          ]
        }
      ],
      "source": [
        "print('Number of distinct users:', filtered_users.select('User-ID').distinct().count())\n",
        "print('Number of distinct users in ratings:', ratings.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uO7Z1IJBkAP",
        "outputId": "f1532bd5-3b89-4857-f276-165d3395f3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 104634\n"
          ]
        }
      ],
      "source": [
        "check_users = filtered_users.join(ratings, 'User-ID', 'inner')\n",
        "print('Number of distinct users in ratings:', check_users.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiRjV8eRBkAP",
        "outputId": "53401958-9bdc-45cb-d14b-905ee6c13ba5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 92100\n"
          ]
        }
      ],
      "source": [
        "check_book = ratings.join(books, 'ISBN', 'inner')\n",
        "print('Number of distinct users in ratings:', check_book.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp7gbrSuBkAP",
        "outputId": "8c2401cf-25d9-4178-920a-948310ee7c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 125:======================================>                  (2 + 1) / 3]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users in ratings: 91548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "check_total = check_users.join(check_book, 'User-ID', 'inner')\n",
        "print('Number of distinct users in ratings:', check_total.select('User-ID').distinct().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIs2fwB6BkAP",
        "outputId": "8f8203ec-e013-4f2d-86d1-3261311f5edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ratings count before filtering: 1149780\n",
            "Ratings count after filtering: 1139222\n"
          ]
        }
      ],
      "source": [
        "# Keep only ratings from users that exist in filtered_users\n",
        "print(\"Ratings count before filtering:\", ratings.count())\n",
        "ratings = ratings.join(filtered_users.select(\"User-ID\"), \"User-ID\", \"inner\")\n",
        "\n",
        "print(\"Ratings count after filtering:\", ratings.count())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fusxiAkMBkAQ"
      },
      "source": [
        "## Xử lý users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kqrBVtGBkAQ",
        "outputId": "e352c8d1-c86c-40f4-c5d0-600cb67c1132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final feature set schema:\n",
            "root\n",
            " |-- User-ID: integer (nullable = true)\n",
            " |-- mean_pub_year: double (nullable = true)\n",
            " |-- median_pub_year: double (nullable = true)\n",
            " |-- std_dev_pub_year: double (nullable = true)\n",
            " |-- mean_author_encoded_age: double (nullable = true)\n",
            " |-- mean_publisher_encoded_age: double (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample of processed data:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 177:=============>                                         (6 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "|User-ID|     mean_pub_year|median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "|     53|1990.5714285714287|         1992.0| 6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|\n",
            "|     78|            1988.0|         1988.0|              NULL|           32.572265625|         30.73865414710485|  18|\n",
            "|     85|            2001.0|         2001.0|1.4142135623730951|                33.2375|          35.0144213318442|  41|\n",
            "|    133|1991.6666666666667|         1991.0| 2.081665999466133|     41.549257105943155|         38.78149527887378|  45|\n",
            "|    243| 1997.506329113924|         1999.0| 5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Cleaning - Convert Year-Of-Publication to integer\n",
        "from pyspark.sql.functions import col, when, avg, stddev, mean, median\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Step 2: Join books with ratings to get user reading history\n",
        "user_book_history = ratings.join(books, \"ISBN\", 'inner')\n",
        "\n",
        "# Step 3: Calculate numerical features from Year-Of-Publication\n",
        "user_numerical_features = user_book_history.groupBy(\"User-ID\").agg(\n",
        "    mean(\"Year-Of-Publication\").alias(\"mean_pub_year\"),\n",
        "    median(\"Year-Of-Publication\").alias(\"median_pub_year\"),\n",
        "    stddev(\"Year-Of-Publication\").alias(\"std_dev_pub_year\")\n",
        ")\n",
        "\n",
        "# Step 4: Calculate target encoding for Book-Author\n",
        "# First, get average age per author from users with known ages\n",
        "author_age_mapping = user_book_history.join(filtered_users.filter(col(\"Age\").isNotNull()), \"User-ID\") \\\n",
        "    .groupBy(\"Book-Author\") \\\n",
        "    .agg(avg(\"Age\").alias(\"author_encoded_age\"))\n",
        "\n",
        "# Apply author encoding to all books\n",
        "books_with_author_encoding = books.join(author_age_mapping, \"Book-Author\", \"left\")\n",
        "\n",
        "# Calculate mean author encoding per user\n",
        "user_author_features = user_book_history.join(books_with_author_encoding, [\"ISBN\", \"Book-Author\"]) \\\n",
        "    .groupBy(\"User-ID\") \\\n",
        "    .agg(mean(\"author_encoded_age\").alias(\"mean_author_encoded_age\"))\n",
        "\n",
        "# Step 5: Calculate target encoding for Publisher\n",
        "publisher_age_mapping = user_book_history.join(filtered_users.filter(col(\"Age\").isNotNull()), \"User-ID\") \\\n",
        "    .groupBy(\"Publisher\") \\\n",
        "    .agg(avg(\"Age\").alias(\"publisher_encoded_age\"))\n",
        "\n",
        "# Apply publisher encoding to all books\n",
        "books_with_publisher_encoding = books.join(publisher_age_mapping, \"Publisher\", \"left\")\n",
        "\n",
        "# Calculate mean publisher encoding per user\n",
        "user_publisher_features = user_book_history.join(books_with_publisher_encoding, [\"ISBN\", \"Publisher\"]) \\\n",
        "    .groupBy(\"User-ID\") \\\n",
        "    .agg(mean(\"publisher_encoded_age\").alias(\"mean_publisher_encoded_age\"))\n",
        "\n",
        "# Step 6: Combine all features\n",
        "user_features = user_numerical_features \\\n",
        "    .join(user_author_features, \"User-ID\", \"left\") \\\n",
        "    .join(user_publisher_features, \"User-ID\", \"left\")\n",
        "\n",
        "# Step 7: Join with original users DataFrame to get target variable (Age)\n",
        "process_user_age = user_features.join(filtered_users.select(\"User-ID\", \"Age\"), \"User-ID\", \"left\")\n",
        "\n",
        "# Display the final feature set\n",
        "print(\"Final feature set schema:\")\n",
        "process_user_age.printSchema()\n",
        "\n",
        "print(\"\\nSample of processed data:\")\n",
        "process_user_age.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiWHWg_rBkAQ",
        "outputId": "a2068e63-9ee2-4a13-ac5b-5b34b8dacbf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users: 91548\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of distinct users: {process_user_age.select('User-ID').distinct().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUEFDwUPBkAQ",
        "outputId": "01ca40da-fe37-47f7-c2b6-ff9b47f51470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts per column:\n",
            "User-ID: 0 null values\n",
            "mean_pub_year: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "median_pub_year: 0 null values\n",
            "std_dev_pub_year: 51003 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_author_encoded_age: 1551 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_publisher_encoded_age: 394 null values\n",
            "Age: 38261 null values\n",
            "\n",
            "Total rows in dataset: 91548\n",
            "\n",
            "Sample of rows containing null values:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 395:===========================>                          (12 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "|User-ID|     mean_pub_year|median_pub_year| std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|\n",
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "|     53|1990.5714285714287|         1992.0|6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|\n",
            "|     78|            1988.0|         1988.0|             NULL|           32.572265625|         30.73865414710485|  18|\n",
            "|    243| 1997.506329113924|         1999.0|5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|\n",
            "|    463|            1995.0|         1995.0|             NULL|     25.023255813953487|                      24.0|  20|\n",
            "|    472|            1993.0|         1994.5|5.291502622129182|     36.395833333333336|         38.68284521048285|NULL|\n",
            "+-------+------------------+---------------+-----------------+-----------------------+--------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for null values in each column\n",
        "from functools import reduce\n",
        "print(\"Null value counts per column:\")\n",
        "for column in process_user_age.columns:\n",
        "    null_count = process_user_age.filter(col(column).isNull()).count()\n",
        "    print(f\"{column}: {null_count} null values\")\n",
        "\n",
        "# Check total null values in the dataset\n",
        "total_rows = process_user_age.count()\n",
        "print(f\"\\nTotal rows in dataset: {total_rows}\")\n",
        "\n",
        "# Show sample of rows with null values\n",
        "print(\"\\nSample of rows containing null values:\")\n",
        "process_user_age.filter(\n",
        "    reduce(lambda x, y: x | y, [col(c).isNull() for c in process_user_age.columns])\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfdoZPPUBkAQ",
        "outputId": "702449ef-84c1-46be-c2fe-b51ad2d3dc31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null value counts after filling:\n",
            "User-ID: 0 null values\n",
            "mean_pub_year: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 11:10:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "median_pub_year: 0 null values\n",
            "std_dev_pub_year: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_author_encoded_age: 0 null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_publisher_encoded_age: 0 null values\n",
            "Age: 38261 null values\n"
          ]
        }
      ],
      "source": [
        "# Fill null values with median for all columns except 'Age'\n",
        "from pyspark.sql.functions import col, when, lit\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Get median values for each column\n",
        "median_values = {}\n",
        "for column in process_user_age.columns:\n",
        "    if column not in ['User-ID', 'Age']:  # Skip User-ID and Age columns\n",
        "        median = process_user_age.approxQuantile(column, [0.5], 0.01)[0]\n",
        "        median_values[column] = median\n",
        "\n",
        "# Fill null values with median for each column\n",
        "for column, median in median_values.items():\n",
        "    process_user_age = process_user_age.withColumn(\n",
        "        column,\n",
        "        when(col(column).isNull(), lit(median)).otherwise(col(column))\n",
        "    )\n",
        "\n",
        "# Verify null values after filling\n",
        "print(\"Null value counts after filling:\")\n",
        "for column in process_user_age.columns:\n",
        "    null_count = process_user_age.filter(col(column).isNull()).count()\n",
        "    print(f\"{column}: {null_count} null values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7_wby1WBkAQ",
        "outputId": "ebbc6299-d4ee-4884-feb1-c34bbd2d5410"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 692:====================>                                  (9 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "|summary|           User-ID|     mean_pub_year|   median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age|               Age|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "|  count|             91548|             91548|             91548|             91548|                  91548|                     91548|             53287|\n",
            "|   mean|139556.91627343034|1949.1277815156284|1959.3848418316074|40.343129660885744|      36.82757096297919|         36.85294459759175| 35.73308686921763|\n",
            "| stddev| 80535.95039413006| 263.8792145947239|259.62377141512445| 171.3477207070269|      4.729349031229777|        3.0107911217025856|13.659348477173793|\n",
            "|    min|                 2|               0.0|               0.0|               0.0|                   10.0|                      13.0|                10|\n",
            "|    max|            278854|            2038.0|            2038.0|1417.0419894978413|                   80.0|                      80.0|                80|\n",
            "+-------+------------------+------------------+------------------+------------------+-----------------------+--------------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "process_user_age.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcrEbfsUBkAQ",
        "outputId": "9a3c65a2-7915-4352-aee1-ef18e63f6bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_pub_year: 0\n",
            "Null values in median_pub_year: 0\n",
            "Null values in std_dev_pub_year: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_author_encoded_age: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null values in mean_publisher_encoded_age: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 11:11:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
            "25/05/27 11:11:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "|User-ID|     mean_pub_year|median_pub_year|  std_dev_pub_year|mean_author_encoded_age|mean_publisher_encoded_age| Age|            features|        prediction|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "|     53|1990.5714285714287|         1992.0| 6.477065767180455|      42.33489304812834|         37.53558997111198|NULL|[1990.57142857142...|  49.6981632784957|\n",
            "|    243| 1997.506329113924|         1999.0| 5.806436344135757|      38.51132047009556|         37.74714772981151|NULL|[1997.50632911392...| 41.44200527815379|\n",
            "|    472|            1993.0|         1994.5| 5.291502622129182|     36.395833333333336|         38.68284521048285|NULL|[1993.0,1994.5,5....|30.769982848471894|\n",
            "|    513|            1989.0|         1989.0|4.6583258795408335|      33.86682808716707|         36.04533152909337|NULL|[1989.0,1989.0,4....|29.927240045755855|\n",
            "|    516|            1995.0|         1995.0|4.6583258795408335|     38.723395270270274|         38.95392491467577|NULL|[1995.0,1995.0,4....| 38.66440419122449|\n",
            "+-------+------------------+---------------+------------------+-----------------------+--------------------------+----+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8564:==============================>                      (14 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Square Error on training data: 10.63405841382434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, first\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Prepare features for Gradient Boosting\n",
        "feature_cols = [c for c in process_user_age.columns if c not in ['User-ID', 'Age']]\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol='features'\n",
        ")\n",
        "# Check for null values in feature columns\n",
        "for feature in feature_cols:\n",
        "    null_count = process_user_age.filter(col(feature).isNull()).count()\n",
        "    print(f\"Null values in {feature}: {null_count}\")\n",
        "# Split data into training and prediction sets\n",
        "train_data = process_user_age.filter(col('Age').isNotNull())\n",
        "predict_data = process_user_age.filter(col('Age').isNull())\n",
        "\n",
        "# Create and train Gradient Boosting model\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol='features',\n",
        "    labelCol='Age',\n",
        "    maxIter=100,\n",
        "    maxDepth=5,\n",
        "    stepSize=0.1\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(predict_data)\n",
        "\n",
        "predictions.show(5)\n",
        "\n",
        "# Evaluate model performance on training data\n",
        "train_predictions = model.transform(train_data)\n",
        "evaluator = RegressionEvaluator(labelCol='Age', predictionCol='prediction')\n",
        "rmse = evaluator.evaluate(train_predictions)\n",
        "print(f\"Root Mean Square Error on training data: {rmse}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN2PNC3VBkAQ",
        "outputId": "0a122af6-1e10-48ed-b779-c276f6e47efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users with null ages that weren't predicted: 72924\n",
            "\n",
            "Sample of users with missing predictions:\n",
            "+-------+\n",
            "|User-ID|\n",
            "+-------+\n",
            "|   NULL|\n",
            "|      1|\n",
            "|      3|\n",
            "|      5|\n",
            "|      7|\n",
            "+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Number of predicted users not in original null age set: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for missing User-IDs between filtered_users and predictions\n",
        "null_age_users = filtered_users.filter(col('Age').isNull()).select('User-ID')\n",
        "predicted_users = predictions.select('User-ID')\n",
        "\n",
        "# Find users with null ages that weren't predicted\n",
        "missing_predictions = null_age_users.join(\n",
        "    predicted_users,\n",
        "    on='User-ID',\n",
        "    how='left_anti'\n",
        ")\n",
        "\n",
        "print(\"Number of users with null ages that weren't predicted:\", missing_predictions.count())\n",
        "if missing_predictions.count() > 0:\n",
        "    print(\"\\nSample of users with missing predictions:\")\n",
        "    missing_predictions.show(5)\n",
        "\n",
        "# Find predicted users that weren't in the original null age set\n",
        "extra_predictions = predicted_users.join(\n",
        "    null_age_users,\n",
        "    on='User-ID',\n",
        "    how='left_anti'\n",
        ")\n",
        "\n",
        "print(\"\\nNumber of predicted users not in original null age set:\", extra_predictions.count())\n",
        "if extra_predictions.count() > 0:\n",
        "    print(\"\\nSample of extra predicted users:\")\n",
        "    extra_predictions.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUqpDpApBkAQ",
        "outputId": "1fb8999b-6a1c-4ebb-86b4-d4934e4fa907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated user data with predicted ages:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8685:========================>                            (11 + 10) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+\n",
            "|User-ID|Age|\n",
            "+-------+---+\n",
            "|     53| 49|\n",
            "|     78| 18|\n",
            "|     85| 41|\n",
            "|    133| 45|\n",
            "|    243| 41|\n",
            "+-------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.dataframe import DataFrame as SparkDataFrame\n",
        "\n",
        "filtered_users = filtered_users.join(ratings, 'User-ID', 'inner').select('User-ID', 'Age')\n",
        "\n",
        "# Convert filtered_users to PySpark DataFrame if it's a Pandas DataFrame\n",
        "if not isinstance(filtered_users, SparkDataFrame):\n",
        "    filtered_users = spark.createDataFrame(filtered_users)\n",
        "\n",
        "# Now proceed with the join operation\n",
        "predictions = predictions.withColumn(\"prediction\", col(\"prediction\").cast(\"integer\"))\n",
        "\n",
        "# Join predictions with filtered_users to update null ages\n",
        "updated_users = filtered_users.join(\n",
        "    predictions.select('User-ID', 'prediction'),\n",
        "    on='User-ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "updated_users = updated_users.withColumn(\n",
        "    'Age',\n",
        "    when(col('Age').isNull(), col('prediction'))\n",
        "    .otherwise(col('Age'))\n",
        ")\n",
        "\n",
        "# Drop the prediction column as it's no longer needed\n",
        "updated_users = updated_users.drop('prediction')\n",
        "updated_users = updated_users.select('User-ID', 'Age').distinct()\n",
        "# Drop rows where Age is null\n",
        "updated_users = updated_users.dropna(subset=['Age'])\n",
        "\n",
        "# Show the results\n",
        "print(\"Updated user data with predicted ages:\")\n",
        "updated_users.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB0E_hMoBkAQ",
        "outputId": "8f66ae98-2066-44f7-ebd8-ac52117b10c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 8741:===============================================>      (21 + 3) / 24]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct users: 99636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "print(f\"Number of distinct users: {updated_users.select('User-ID').distinct().count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yi5h1GFBkAQ",
        "outputId": "654572d5-47ad-46b3-e1d1-682bbb0079a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAIjCAYAAABswtioAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAjC9JREFUeJzt3Qd0nMXVxvFHvfdq2ZZ77wVMM2BsbMAQWkggGAgthEBoCS10AiFAIHRISCj56ITeTLHBGGzcu+VeZMu2itV72f3OjCxFcre80rsr/X/nrPfd3VezV56VtHdn5o6f2+12CwAAAADQ5vzb/ikBAAAAAAYJGQAAAAA4hIQMAAAAABxCQgYAAAAADiEhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQB0IPfee6/8/Pza5LlOPPFEe2nw3Xff2ef+73//2ybP/+tf/1rdu3eXNystLdUVV1yh1NRU+39zww03OB0SAKCNkZABgI965ZVX7Jv4hktoaKjS0tI0adIkPfXUUyopKfHI82zbts0mcosXL5a38ebYDsZf/vIX249XX321/u///k8XXXTRPs81fXzttdfu9TGT5JrHTdLrbW655RYb2y9/+UunQwEArxTodAAAgMNz//33q0ePHqqpqdGOHTvsm3Iz0vL444/r448/1tChQxvPvfPOO3XbbbcdctJz33332dGm4cOHH/TXffXVV2pt+4vtxRdflMvlkjebPn26jjrqKN1zzz1qj9xut958803bP5988on9kCAqKsrpsADAqzBCBgA+7tRTT9WUKVN06aWX6vbbb9eXX36pb775Rjk5OfrZz36mioqKxnMDAwPtSFprKi8vt9fBwcH24pSgoCCFhITIm5k+io2NlS9q6Of9MR8ObN26VS+99JJqa2v1/vvvt0lsAOBLSMgAoB066aSTdNddd2nz5s167bXX9ruG7Ouvv9Zxxx1nE4PIyEj169dPf/rTnxrfUB9xxBH22CR8DdMjzTQ7w6wRGzx4sBYsWKDjjz9e4eHhjV+7+xqyBnV1dfYcs24qIiLCJo1btmxpdo4ZUTFrwHbXtM0Dxba3NWRlZWX6wx/+oK5du9pkzXyvf/vb3+xIzt6mB3744Yf2+zPnDho0SFOnTj3oROvyyy9XSkqKTYCHDRumV199dY/1dBs3btRnn33WGPumTZvkKWvXrtW5555r/59NDF26dNH555+voqKiZueZ18eoUaMUFham+Ph4e87u/bG/ft6f119/XQMHDtS4ceM0YcIEe3tvzOvUvA7M6yE5OVk33nij/WBhb9Mw58yZo1NOOUUxMTE2jhNOOEE//vhjs3PMSJwZJTb9b/rOtHnyySdr4cKFh/A/CABtgymLANBOmfVI5k2zmTp45ZVX7vWcFStW6PTTT7fTGs3UR/Pmdd26dY1vcAcMGGDvv/vuu/Wb3/xGY8eOtfcfc8wxjW3s3LnTjtKZN/JmpM4kIfvz4IMP2jfat956q01cnnjiCftm3awDM0nBwTqY2JoySZd50//tt9/aZMlMcTRv+m+++WZlZWXp73//e7Pzf/jhBzui87vf/c5OszPr8kyCk5mZqYSEhH3GZUYkTQJj/h9NUmemk7777rs2QSwsLNT1119vYzdrxkziYRIlkyQaSUlJ8oTq6mq7lrCqqkq///3vbVJmvsdPP/3UxmCSmYa+MIn7L37xC1tcJDc3V08//bRNuhYtWtRs9O5Q+9k893vvvdf4vV1wwQU2cTbTak08TZNk8wHC9u3b7f+NeeyNN96w/bS3KZ4mBpNAmmme/v7+evnll+3Xz5w5U0ceeaQ977e//a1dV2f+/01CaGI3/ZmRkaGRI0d65P8YADzGDQDwSS+//LIZ1nHPmzdvn+fExMS4R4wY0Xj7nnvusV/T4O9//7u9nZubu882TPvmHPN8uzvhhBPsYy+88MJeHzOXBt9++609t3Pnzu7i4uLG+9955x17/5NPPtl4X7du3dyXXHLJAdvcX2zm6007DT788EN77gMPPNDsvJ///OduPz8/97p16xrvM+cFBwc3u2/JkiX2/qefftq9P0888YQ977XXXmu8r7q62n300Ue7IyMjm33vJr7Jkyfvt72mMV1zzTV7fezdd9+1j5v/Y2PRokX2trl/XzZt2uQOCAhwP/jgg83uX7ZsmTswMLDZ/fvr533573//a79m7dq19rb5vkNDQ+1rrqnHHnvMnmf6p0FFRYW7f//+zb4nl8vl7tOnj3vSpEn2uEF5ebm7R48e7pNPPrnZ635f/1cA4G2YsggA7ZiZgri/aosNIyAfffRRiwtgmFE1M/JxsC6++OJmhR1+/vOfq1OnTvr888/Vmkz7AQEBuu6665rdb0ZwTL7zxRdfNLvfjNr16tWr8bYZRYyOjtaGDRsO+DxmlMeMCDVdz2ae15S5nzFjhlpbwwiYGQHc11ovM/pn+tyMjuXl5TVeTOx9+vTZY4TqUPvZTE8cPXq0evfubW+bPp88efIe0xbNNNDOnTvb0csGZorl7qO6ZgTVTMP81a9+ZUe8GuI1I2zjx4/X999/3/gaNq9rM7XRFH0BAG9HQgYA7ZhJAPZX1c6UIj/22GPtdDUzBc1MR3vnnXcOKTkzb6YPpXiHebPflJm+aN60e3L91L7WKZltAXb//zDTBxsebyo9PX2PNuLi4lRQUHDA5zHfo5lOdzDP40kN6wPNNMmbbrpJ//rXv5SYmGinLz777LPN1o+Z5MYkoiZWM1Wy6cVM7TPTSVvaz2ZapElMzfouM3Wz4WJea/Pnz9eaNWsazzX/Hybx3X1tY0Mi1zRe45JLLtkjXvN9mimSDd/fI488ouXLl9u1gmYao1k7eaBEGgCcwhoyAGinTHU78wZ19ze2TZk1W2ZkwYyGmOISZrTi7bfftmtyzNozM6J0IIey7utg7WvzalMQ5GBi8oR9Pc/uBUDaihmhaloxs6mGUbCmFTQfe+wxu27NjH6avjQjdA899JB++uknu27NJN3m/9mMDO7tezWjqy3tZ7NmziRIJgZz2Z0ZJTPbFRyKhg8JHn300X1uv9AQsxn1M2sKP/jgA/u9m695+OGH7aigWYMGAN6EhAwA2ilTNMIwoyP7Y0ZyzJQvczF7l5nNiu+44w6bpJlpe/tKjlqqYaSjaYJjRk+a7pdmRqLMKMvuzGhKz549G28fSmzdunWz2wHsvhfWqlWrGh/3BNPO0qVLbQLRdJTscJ/HfN3q1av3+ljD/bu3PWTIEHsx+8/NmjXLjlC98MILeuCBB+yolPm/N6Npffv2lSeZhMtUZdzb/mr/+Mc/bNGOhoTMxLxy5UobS9P+NK+Jphqmj5ppo+Z1eSBmGqwpyGIuZrTPFPMwRUxIyAB4G6YsAkA7ZKrR/fnPf7Zvti+88MJ9npefn7/HfQ2jD2aEwzClyI29JUgt8Z///KfZujZTDc9U2Gv6Rtm8+TYjOaZaYANTIXD3cuyHEttpp51mR9ieeeaZZveb6oomEfDUG3XzPKaSoBlpbGD24DLVC80IjpnG19J2zf+JKT3flPneTQJk+q2hemFxcbF9zqZMYmYSxIZ+Peecc+zImEmMdh/1M7fNOq2WMH1kRl3NKJVZH7j7xaxDM8mWWePV8IGBqQBpNjFvUFlZaTf2bspUVjSvC7NNgZmKuztTIdIwfbx7aX9T9t5MV2343gHAmzBCBgA+zkw5M6Mv5g14dna2TcbM3mJm5MG8yd3fRtCmbLx582yKLZjzzUjCc889Z6e0mb3JDPMm2BRJMCMrZmTJJEFjxoyxyV5LmL2uTNvmjbmJ15S9N9MqmxZxMGvaTKJm9psyb+zXr19v98tqWmTjUGM744wz7H5YZvTPrFcze4OZ6WxmSp/Zs2r3tlvKlOA3o0BmuqBJnsxeWOZ7MVsJmO91f2v69ue2226zUwFNSfqrrrpK/fv3t0UrzL5rJqE15d8bmNeAKfl+3nnn2dEv89owI6YmATOl+w3z/ZqRMrOZuPn/OOuss2xsZm80M9XPfB9//OMfDzlOM/rVsMXAvhJLs0G5SSJNX5nvxSTJpgiKKXtvRrbMYw2v24ZRM5NMmrViJnE2e8KZ149Z12aSOTOaa0bOPvnkE5vsm9evSf5MH5sk2IyMzps3b6/TJwHAcU6XeQQAHF7Z+4aLKdOemppqy3+bEvJNy6vvq+z9tGnT3GeeeaY7LS3Nfr25vuCCC9xr1qxp9nUfffSRe+DAgbYcetMy86Yc+qBBg/Ya377K3r/55pvu22+/3Z2cnOwOCwuzZd83b968x9ebcuimRH5ISIj72GOPdc+fP3+PNvcX2+5l742SkhL3jTfeaL/PoKAgW0b90UcfbVZGfX8l5vdVjn932dnZ7ksvvdSdmJho/1+HDBmy19L8h1L23ti6dav7iiuusP8v5vuNj493n3766e6ffvqp2XkbNmxwX3bZZe5evXrZUvPmvHHjxrm/+eabPdp877333Mcdd5w7IiLCXky5efO9r169uvGc/fXz7sz3mp6evt9zTjzxRNv/NTU1jfGa/wfzekhKSnL/4Q9/sHGZftj9ezMl/c855xx3QkKCfW2Y/8Nf/OIX9rVsVFVVuW+++Wb3sGHD3FFRUfZ7MsfPPffcQcUPAG3Nz/zjdFIIAADQlBlNNBtnm+I0ZiQMANorEjIAAOAoUz2yaRVHs4ZsxIgRdj1Y0xL5ANAesYYMAAA4yhQYMfu+mcIkpiCHWS9o1kXuvok0ALRHJGQAAMBRptKiKdhhEjAzKjZw4EC99dZbduNyAGjvmLIIAAAAAA5hHzIAAAAAcAgJGQAAAAA4hDVkHuJyuewGnWZTzYZNLAEAAAB0PG63225Un5aWZje23x8SMg8xyVjXrl2dDgMAAACAl9iyZYu6dOmy33NIyDzEjIw1/KdHR0c7PlqXm5urpKSkA2bkaBv0ifehT7wL/eF96BPvQ594F/rD+7i8qE+Ki4vtYE1DjrA/JGQe0jBN0SRj3pCQmU01TRxOvxhRjz7xPvSJd6E/vA994n3oE+9Cf3gflxf2ycEsZfKOSAEAAACgAyIhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQAAAAA4hIQMAAAAABxCQgYAAAAADiEhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQAAAAB0xITs+++/1xlnnKG0tDT5+fnpww8/bPa42+3W3XffrU6dOiksLEwTJkzQ2rVrm52Tn5+vCy+8UNHR0YqNjdXll1+u0tLSZucsXbpUY8eOVWhoqLp27apHHnlkj1jeffdd9e/f354zZMgQff755630XQMAAACAFyRkZWVlGjZsmJ599tm9Pm4Sp6eeekovvPCC5syZo4iICE2aNEmVlZWN55hkbMWKFfr666/16aef2iTvN7/5TePjxcXFmjhxorp166YFCxbo0Ucf1b333qt//vOfjefMmjVLF1xwgU3mFi1apLPOOsteli9f3sr/AwAAAAA6Mj+3GYbyAmaE7IMPPrCJkGHCMiNnf/jDH/THP/7R3ldUVKSUlBS98sorOv/885WRkaGBAwdq3rx5Gj16tD1n6tSpOu2007R161b79c8//7zuuOMO7dixQ8HBwfac2267zY7GrVq1yt7+5S9/aZNDk9A1OOqoozR8+HCbDB4Mk/jFxMTYGM1onZNcLpdycnKUnJwsf39mpXoD+sT70Cfehf7wPvSJ96FPvAv94X1cXtQnh5IbBMpLbdy40SZRZppiA/NNjRkzRrNnz7YJmbk20xQbkjHDnG86wIyonX322fac448/vjEZM8wo28MPP6yCggLFxcXZc2666aZmz2/O2X0KZVNVVVX20vQ/veGFYC5OMs9vElqn48D/0Cfehz45fFu2bFFeXp5H2jJ9YWY/mA/TzAd0RmJiop1mDmfwM+J96BPvQn94H5cX9cmhxOC1CZlJxgwzItaUud3wmLk2GXBTgYGBio+Pb3ZOjx499mij4TGTkJnr/T3P3jz00EO677779rg/Nze32ZRKp14AJhs3L0inPx1APfrE+9Anh8f8rrv6d79TdZMPpg6HScJ69+6tdevW2T4xgkNC9PxzzykpKckjz4FDw8+I96FPvAv94X1cXtQnJSUlvp+Qebvbb7+92aiaGSEzn+SaNw7eMGXRvLkxsTj9YkQ9+sT70CeHJysrS7NnzdKvbn5Yyem9PNJmfKiUemz9cU7mer3x6K2qrq7e44M3tA1+RrwPfeJd6A/v4/KiPjGFAn0+IUtNTbXX2dnZtspiA3PbrO1qOMfME22qtrbWVl5s+Hpzbb6mqYbbBzqn4fG9CQkJsZfdmc53+gVgmBejt8SCevSJ96FPDu//zvzhM8lYlz6DDr9Bt1vhdaUKD4g0jTf7w0r/OIefEe9Dn3gX+sP7+HlJnxzK83vtq8dMMzQJ0bRp05qNQpm1YUcffbS9ba4LCwtt9cQG06dPt3/EzVqzhnNM5cWamprGc0xFxn79+tnpig3nNH2ehnMangcAAAAAWoOjCZnZL2zx4sX20lDIwxxnZmba7PaGG27QAw88oI8//ljLli3TxRdfbCsnNlRiHDBggE455RRdeeWVmjt3rn788Udde+21tuCHOc/41a9+ZQt6mJL2pjz+22+/rSeffLLZdMPrr7/eVmd87LHHbOVFUxZ//vz5ti0AAAAAaC2OTlk0Sc+4ceMabzckSZdccoktbX/LLbfYcvRmXzEzEnbcccfZxKnpnMzXX3/dJk7jx4+3Q4Pnnnuu3busaWXGr776Stdcc41GjRplq3aZzaab7lV2zDHH6I033tCdd96pP/3pT+rTp4+tsDh48OA2+78AAAAA0PE4mpCdeOKJjdW09saMkt1///32si+moqJJpvZn6NChmjlz5n7POe+88+wFAAAAANqK164hAwAAAID2joQMAAAAABxCQgYAAAAADiEhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQAAAAA4hIQMAAAAABxCQgYAAAAADiEhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcEigU08MAL4iMzNTeXl5rdJ2YmKi0tPTW6VtAADg/UjIAOAAyVj/AQNUUV7eKu2HhYdrVUYGSRkAAB0UCRkA7IcZGTPJ2IW3PqqU9F4ebTs7c71ef/hm+xwkZAAAdEwkZABwEEwy1qXPIKfDAAAA7QxFPQAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQAAAAA4hIQMAAAAABxCQgYAAAAADiEhAwAAAACHkJABAAAAgENIyAAAAADAISRkAAAAAOAQEjIAAAAAcAgJGQAAAAA4hIQMAHzIztIqzVqfp6LyGqdDAQAAHhDoiUYAAK3v21U5uv6tRSqurJWfn9QvJUpjesTrwqO6qW9KlNPhAQCAFmCEDAC8nMvl1hPfrNFlr86zyVhMWJDcbmnVjhK9Onuzzn1+lpZnFTkdJgAAaAFGyADAi5VX1+raNxZp+qoce/uio7rpztMHqLiiVvM25evFmRu0KLNQF780V+9cdbR6J0c6HTIAADgEjJABgBd7ato6m4yFBPrrsfOG6c9nDVZIYICSokJ02pBOevWyIzWkc4zyy6o15V9ztCW/3OmQAQDAISAhAwAvtb2oQi//uNEeP3XBCJ07qsse50SHBtmkrE9ypHYUV2rKv+fYwh8AAMA3kJABgJf6+9drVFXr0pHd4zVxYMo+z4uPCNZrV4xReny4Nu8s158/XdmmcQIAgJYjIQMAL7Qmu0T/XbDVHt92Wn/5mbKK+5ESHaqnLxghfz/pw8XbNGNNbhtFCgAADgcJGQB4oYe/WCWXWzp1cKpGpscd1NcM6xqrXx/Twx7f8cEyWxAEAAB4NxIyAPAyczbs1LRVOQrw99PNk/od0tf+YWJfdY4N09aCCj35zdpWixEAAHgGCRkAeJlHvlxtr88/oqt6Jh1aGfuIkED9+axB9vhfP2xkfzIAALwcCRkAeNnasQWbCxTo76frx/dpURsn9U/R5KGdVOdy26mLZmNpAADgnUjIAMCLvL8wy16P65+s5OjQFrdzzxkDFRkSqCVbi/Txkm0ejBAAAHgSCRkAeAkzovXhovqE7NyRnQ+rreSoUF19Yi97/MjUVaqsqfNIjAAAwLNIyADAS8xev9Nu7hwTFmRHyA7X5cf1UFpMqLYVVeqlXRtMAwAA70JCBgBe4v2F9fuOnTGsk0ICAw67vdCgAN18Sn2Vxue+Xa+80qrDbhMAAHgWCRkAeIGyqlp9sXyHPT5nZBePtXvmsM4a0jlGpVW1euKbNR5rFwAAeAYJGQB4ganLd6iipk49EiM0omusx9r19/fTHZMH2OM3527RupwSj7UNAAAOHwkZAHiB9xfVT1c8Z0Rn+fn5ebTto3om6OSBKbZoyEOfr/Jo2wAA4PCQkAGAw/LK6zRr/U57fNaIw6uuuC+3n9rf7m02bVWOZq3La5XnAAAAh46EDAAc9kNmhdxu6cge8eoaH94qz9EzKVIXjkm3xw98lsFm0QAAeAkSMgBw2Lxtlfb69KGdWvV5rp/QV1GhgVq5vVjv79rvDAAAOIuEDAAc5B8WrdU7a+zx+AEprfpc8RHBunZcb3v8ty9Xq6KazaIBAHAaCRkAOCis12iZ2YMDO0Wrc2xYqz/fJcd0t89jNqB+ceaGVn8+AACwfyRkAOCgsN5j7PWEga07OtZ0s+hbT+1vj1+YsV45JfXTJQEAgDNIyADAIXVuKazHSHs8YUBymz3vGUM7aXjXWJVX1+nvX7NZNAAATiIhAwCH5Fb6yT84TPFh/hqcFtNmz2v2Obtz12bRb8/botU72CwaAACnkJABgEO2V9T/Ch7dKVT+/p7dDPpARneP16mDU+36tb98ntGmzw0AAP6HhAwAHOB2u/+XkKWFOBLDraf0V1CAn2asydX3a3IdiQEAgI6OhAwAHJBbWqWKOj+5qis1JNmZhKx7YoQuOqq7PTajZHVsFg0AQJsjIQMAB2zMLbPXlZsWKSSwbacrNnXd+N6KCQvSqh0ldj0ZAABoWyRkAOCADXn1CVn5ujmOxhEbHqzrx/exxw9PXaW80ipH4wEAoKMhIQOANlZSWaOcEpP4uFWxfp7T4ejio7vZjamLKmoo8AEAQBsjIQOANrZx1+hYfLBbrvIip8NRYIC/Hjx7sPz8pPcXZmnW+jynQwIAoMMIdDoAAOioCVlamEuLJGVktN6oVGJiotLT0w943oj0OF04Jl2v/ZSpOz9cri+uH6uQwIBWiwsAANQjIQOANlRd69KWggp7HFWz015PmTKl1Z4vLDxcqzIyDiopu3lSf01dnq0NuWX6x4wNum7X2jIAANB6SMgAoA1l5pfb8vKmsqF/eb69b/JVd6jf0FEef67szPV6/eGblZeXd1AJmYnprtMH6Pq3FuuZb9fZjaP7pER5PC4AAPA/JGQA4MB0xR6JEfLbVWU+Ia2buvQZJG/ws2Fp+mBRlr5bnas/vrtE7119jF1jBgAAWgd/ZQGgjbjc7saErGdihLyRn5+f/nrOUEWHBmrJ1iK9MGO90yEBANCukZABQBvZUVSpipo6BQf6Ky02TN4qNSZU9/6sfsTuyWlrtXJbsdMhAQDQbpGQAUAbaRgd654QrgB/P3mzs0d01sSBKaqpc+sP7y6xxUgAAIDnkZABQBvZ0DhdMVLezkxdfPDsIYoLD1LG9mL9/Zs1TocEAEC7REIGAG2gsLxa+WXVMgNjZoTMFyRFhegvZw+xx89/t17frspxOiQAANodEjIAaMPRMbN2LCTIdzZcPnVIJ11ydDd7fOM7i7WtsH4PNQAA4BkkZADQBtbllNrrXkneP11xd3+aPEBDu8SosLxG17yxkPVkAAB0lISsrq5Od911l3r06KGwsDD16tVLf/7zn+V2uxvPMcd33323OnXqZM+ZMGGC1q5d26yd/Px8XXjhhYqOjlZsbKwuv/xylZbWvzlqsHTpUo0dO1ahoaHq2rWrHnnkkTb7PgG0b6WVtdpeVGmPe/tgQhYSGKBnfzXSlsJflFmoh6eucjokAADaDa9OyB5++GE9//zzeuaZZ5SRkWFvm0Tp6aefbjzH3H7qqaf0wgsvaM6cOYqIiNCkSZNUWVn/5scwydiKFSv09ddf69NPP9X333+v3/zmN42PFxcXa+LEierWrZsWLFigRx99VPfee6/++c9/tvn3DKD9WZdb/wFQp5hQRYYGyhd1jQ/X384bZo///cNGfb2h3OmQAABoF7z6ncGsWbN05plnavLkyfZ29+7d9eabb2ru3LmNo2NPPPGE7rzzTnue8Z///EcpKSn68MMPdf7559tEburUqZo3b55Gjx5tzzEJ3Wmnnaa//e1vSktL0+uvv67q6mq99NJLCg4O1qBBg7R48WI9/vjjzRI3AGiJtTkl9rpPsu+NjjU1cVCqfn9Sbz09fZ3+saBIYb2OcDokAAB8nlcnZMccc4wdpVqzZo369u2rJUuW6IcffrCJkrFx40bt2LHDTlNsEBMTozFjxmj27Nk2ITPXZppiQzJmmPP9/f3tiNrZZ59tzzn++ONtMtbAjLKZEbmCggLFxcXtEVtVVZW9NB1lM1wul704yTy/SVadjgP/Q5/4bp+Yc8zvi103Dvl5yqpqta2wYbpiRLM2zE5kpm2/FrZ9MEz7nnzt3TC+ty3s8d7CLCWeeZvyq/zUxROxmzYaLq0UOw4Nv7e8D33iXegP7+Pyoj45lBi8OiG77bbbbKLTv39/BQQE2DVlDz74oJ2CaJhkzDAjYk2Z2w2Pmevk5ORmjwcGBio+Pr7ZOWad2u5tNDy2t4TsoYce0n333bfH/bm5uc2mSzr1AigqKmr+RhKOok98t0/Mz/OoUaMUHyqF1zVfe3owVmbvqq4YFaSUoCqp7n8f5KTGRdi2U6KDW9T2gZiYTfvme8jJ8VzJ+huPTdHaLdlamivNznOrT0mhEsIP98+JWyHuSsn+/fJrtdhx8Pi95X3oE+9Cf3gflxf1SUlJ/ewYn0/I3nnnHTud8I033micRnjDDTfYaYaXXHKJo7Hdfvvtuummmxpvm8TRFANJSkqyxUOcfjGaTV1NLE6/GFGPPvHdPsnKyrJrS8dWSuEBhz7lcGVeob3ulRKt8t2+fkdBmW174OnVSm1B2weSXynbvilWtPsHU4frluPi9Yvnf5RS++j/FhfozGGdlBId2vIG7eiYVO4faXalbtXYcXD4veV96BPvQn94H5cX9Yn5+9UuErKbb77ZjpKZqYfGkCFDtHnzZjs6ZRKy1NRUe392dratstjA3B4+fLg9Nufs/ulqbW2trbzY8PXm2nxNUw23G87ZXUhIiL3sznS+0y8Aw7wYvSUW1KNPfLNPzDmN0w787OTCQ5qumNUwXTElao+vdzdMr2hB24f6x8nTr7vwoABlv3ufBt/0HxXVSO8tytJpQzqpe0JEyxs1/wcNl1aMHQeP31vehz7xLvSH9/Hzkj45lOf36ldPeXn5Ht+MmbrY8ObITDM0CdO0adOajVSZtWFHH320vW2uCwsL7SetDaZPn27bMGvNGs4xlRdramoazzEVGfv167fX6YoAcCjVFVOiQxQdGqT2xlVeqBNSatU1Lkw1dW59vGSbVmwrcjosAAB8ilcnZGeccYZdM/bZZ59p06ZN+uCDD2xBD1OIoyEDNlMYH3jgAX388cdatmyZLr74Yjul8ayzzrLnDBgwQKeccoquvPJKW53xxx9/1LXXXmtH3cx5xq9+9Stb0MPsT2bK47/99tt68sknm01JBICWbgbdJzlK7VWQv3Tm8M7qlxplZx1+k5GjL5ZvV0V1ndOhAQDgE7x6yqIpT282hv7d735npx2aBOqqq66yG0E3uOWWW1RWVmbL05uRsOOOO86WuW86b9OsQzNJ2Pjx4+2I27nnnmv3LmtamfGrr77SNddcYxeRJyYm2ueg5D2Aw9kMOqugol2Uuz+QAH8/TRqYopiwIM3bmK812aXakl+hcf2S1MdM1QQAAL6ZkEVFRdl9xsxlX8wo2f33328v+2IqKprCIPszdOhQzZw587DiBYAGy7KK7NqwzrFhig5rf9MV9/a7+OieCeqRGKFvVmZrZ1m1Pl++Q+nbinVkj3j7/wAAAHwsIQMAX1Tncmv5rrVUQ7vEqCNJjQ7V+Ud21byNBZq/OV+Z+eX20iU2zCZmXeLCbPIGAADqkZABsDIzM5WXl9dq7ZupwOnp6eoI1ueWqry6TuHBAeqV1L6nK+5NoL+/ju6VoIFp0Zq/KV8rtxdra2GFti7KUqeYUB3ZPV7dEsJJzAAAICED0JCM9R8wQBXl5a32HGHh4VqVkdEhkrIlW+v3HhvcOcaur+qozJqy8QNS7MjYgs0FWr6tWNuLKvXRkm1KjgrRMb0S1O1wyuQDANAOkJABsCNjJhm78NZHlZLey+PtZ2eu1+sP32yfp70nZHmlVdpWWGm30hqS1rGmK+5LVGiQTuyXrCO6x2tBZoGWbS1STkmVPly8TQM7Rev4PokKCfTqor8AALQaEjIAjUwy1qXPIKfD8GlLt9avHeuVGKnIUH7FNhUREqjj+yRpdLc4zd2YryVbi+x0xs07y3RS/2QNZttHAEAHxEeSANp9gY22UlVbp1U7ijtkMY9DER4caEfMfj6qi2LDg1RWXadPlm7X3C1lTocGAECb4+NbAO1ObZ1LHy3epme/W6cNuWWKDg1UfESwEiNDdMrgVF10dDcFtcLaroztJaqpcys+PNhWE8T+mVL4Fx6Zrh/X79TiLYWatqFE7qBQDekS63RoAAC0GRIyAO2Gy+XW+4uy9Mz0tdq0838FSoora+3F3Dd/c4Fe/nGT/nByHx2V5rlfgRXVdZqzcac9HtY1hgqCBykwwN+uIQvwkxZkFmr66lwFBfir49WmBAB0VCRkANpNMnbzf5fqvYVb7e248CBdeXxPnT2is8qq6pRfVm2nEz777TplFVbopneXqm9SmJ65MEJ9U6MP+/l/WJenyhqXEiODNYhiHofEJK/H9kqQu7ZaC7eV66uMbI1JIKEFAHQMJGQAfJ7b7da9n6ywyZgpM3/TyX3162O62yISTZny6+eN6qqXZ23U89+t15rcCp313Cw9dM4QnTm8c4ufP6ugwhanMMb1S+7Qpe4PJymb2DtKFa4AZewo0fydgQqISnA6LAAAWh1FPQD4vEe+XK3/zN5sS80/dt4wXTOu9x7JWIOw4AD97sTe+ubG4zW6a5TdwPn6txbrrg+X26IcLSka8u3qHHs8OC1aabGsHTucpGzCgGS7eXSt20/x469yOiQAAFodCRkAn2amIJrRLuPBs4borBEHN9KVFBWiJ8/uo2vH1e+79n8/bdZ5L8zWlvxD2xx7UWaBdpZVKywoQMf2TmzBd4Cm/P38bAl8P7kV3u8YzdtW6XRIAAC0KhIyAD5r1vo8Pfrlant8x2kD9Ksxh7bpdMP0xpcvPcKWXzd7iJ3+9A+alpF9UF+/IbdUczbm2+OxfRIVGhTQgu8CuzPVMPtEuezxiwuLVV5d63RIAAC0GhIyAD6ppLJGN7+71B5fcGS6LeDRUmbd12fXjdWwrrEqqqjR5a/O11+/WKXSqtp9rlkzGxubvbNqXW71SIxQ/9SoFj8/9jQgpk61RdnKK6/TE9+sdTocAABaDQkZAJ/04GcZtlqi2e/rjskDPLIn1rtXHW2LgRgvzFivox+aplcWFysgOsneV13rUkFZtaYu36HZG3Y2bgA9eUgnytx7WKC/lP/V8/b43z9sVMauoikAALQ3VFkE4HO+XZWjt+Ztscd/O2+YIvdRwONQBQf6696fDdKYHvF2KuSGvDJ9vKZWna/6lz7a4q/azPq1aoYppGhG1gZ3psR9a6nYMF9HdQ7VT1mVeu679Xr6ghFOhwQAgMcxQgbApxSWV+vW9+qnKl52bA8d1dPzpdFPHdJJ39x0gl769WgNTQ6Wn3+ArfpnBAf4KzkqROeM6EIy1gbOG1i/RfTny7bbEVEAANobRsgA+BSztiunpEo9EyN0yyn9Wu15/P1Ntb8UxZYn6Mhxp+qy+19Qr74D7Cga2k6PuCAd3TPBThH9z6xNuv20w5+eCgCAN+GdBQCfsTyrSG/Pr5+q+Ndzh7ZZVcO64hxFBdVPaUTbu/y4Hvb6jbmZKttHoRUAAHwV7y4A+ART2fC+T1bI7ZZ+NixNR/aIdzoktBGzL5mpZFlSWat3dyXkAAC0FyRkAHzCZ8u2a96mAoUG+eu2U/s7HQ7akJk+etmuUbKXZ21SncvtdEgAAHgMCRkAr1dRXaeHPl9lj397Qi+lxYY5HRLa2LkjOysmLEibd5brm4PcuBsAAF9AQgbA6/3z+w22wl5aTKiuOr6X0+HAAeHBgbpwTHrjvmQAALQXJGQAvNq2wgo9P2OdPTYV9sKC26aQB7zPxUd3V4C/n+ZuzNemvDKnwwEAwCNIyAB4fZn7yhqXjugep9OHdnI6HDgoNSZUx/Sq33fukyXbnA4HAACPICED4LXmb8rXx0u2yc9PuueMQfIzB+jQTIVNw7wuTOVNAAB8HQkZAK/kcpky9yvt8S9Hd9XgzjFOhwQvMGlwqt0Pbm1OqVbtKHE6HAAADlvg4TcBAJ7334VbtSyrSFEhgfrDxH5Oh+PTMjIyfKLNgxEdGqRx/ZL05YpsO0o2oFO0I3EAAOApJGQAvE5JZY0embraHl83vo+SokKcDsknFefn2uspU6a02nOUlpaqrf1sWGebkJl1ZLdM6sdUVgCATyMhA+B1npm+TnmlVeqRGKFLjunudDg+q6K02F5PvuoO9Rs6yqNtZ8ydoS9efVKVlZVqa+MHJCsiOEBbCyq0MLNQo7rFtXkMAAB4CgkZAK+yLqe0cZ+pu04fYNcL4fAkpHVTlz6DPNpmduZ6OSU0KEATB6Xqg0VZdpSMhAwA4Mt4pwPAa5iqefd+vEK1LrfG90/WSf1TnA4JXl5t8dOl21Vb53I6HAAAWoyEDIDX+GL5Dv2wLs+Oipky98C+HNcnUXHhQXZq608b8p0OBwCAFiMhA+AVyqtr9cCn9WXuf3tCL6UnhDsdErxYUIC/Th1Sv1H458u3Ox0OAAAtRkIGwGsKeWwrqlSXuDD97sReTocDHzBxYP2U1ukZOWwSDQDwWSRkABy3IbdUL87cYI/vPn2gLdoAHMhRPRMUFhSgHcWVWrm9vqIkAAC+hoQMgPOFPD5ZqZo6t07sl6STd416AAdiEnezlqxhlAwAAF9EQgbAUWaD3+/X5Co4wF/3njGITX5xSEw1TmPaKhIyAIBvIiED4JiK6jr9eVchj98c31PdEyOcDgk+ZtyuhGzJ1kLlllQ5HQ4AAIeMhAyAY577bp2yCivUOTZM14zr7XQ48EEp0aEa0jlGpqbHd6sZJQMA+B4SMgCO2JRXpn/MqC/kcdfpAxQWTCEPtMxJDdMWWUcGAPBBgU4HAKBjuv/Tlaquc2lsn0RNGpR6WG1lZmYqLy/vkIuJVFZWKisra7/r1jIyMg4rNrS+CQNS9OS0tZq5NldVtXUKCSS5BwD4DhIyAG3um5XZmr4qR0EBfrrvZ4dXyMMkY/0HDFBFefkhfZ2/v79GjRqlBQsWyOVyHfD80tLSFseI1jUoLVrJUSHKKanS3I35GtsnyemQAAA4aCRkANpUZU2d7vt0hT2+YmxP9UyKPKz2zMiYScYuvPVRpaQf2obS8aHS2Mr9n5Mxd4a+ePVJO5oG7+Tv72enLb41b4udtkhCBgDwJSRkANrUCzPWa0t+hTrFhOpaDxbyMMlYlz6DDv4L3G6F15UqPCBS2s8IXXbmes8EiFbVmJCtytY9Zwxk+wQAgM+gqAeANpNdWqvnv6tPcO6YPEARIXwmBM8wG0SbvexMsr8xr8zpcAAAOGgkZADazEuLi1VV69IxvRI0eUgnp8NBOxIeHKhR3eLs8Q/rDq3ACwAATiIhA9AmQnuM1LxtVQr099P9Zx5eIQ9gX6Nkxsy1JGQAAN9BQgag1bncUvz4K+3xr4/prt7JUU6HhHbIbKFg/LR+p2rrDlw5EwAAb0BCBqDVbSj1V1BCV0WH+Ou6CX2cDgft1KC0GMWGB6mkqlZLthY6HQ4AAAeFhAxAq6qoqdPKovqNei8YHKno0CCnQ0I7FeDvp2N7MW0RAOBbSMgAtKqfNuxUjctP1TkbNaFHuNPhoIOsI/uBhAwA4CNIyAC0mp2lVVqWVWSP86f9045gAK3puN71CdmiLYUqqaxxOhwAAA6IhAxAq5m5Ls/sv6y0MJeqMpc5HQ46gK7x4eqeEK46l1s/bch3OhwAAA6IhAxAq9haUK7NO8tlBsWGxNY6HQ465LTFXKdDAQDggEjIAHic2+3WrPU7GyvfRVLHA23ouN5JjSO0AAB4OxIyAB63aWe5thdV2jVjR/aIdzocdDBH90qwI7Mbcsu0rbDC6XAAANgvEjIAHh8dm71rdGxYlxhFhgQ6HRI6mJiwIA3rGmuPqbYIAPB2JGQAPGptTqlyS6sUHOCv0d0YHYMzxu6qtsi0RQCAtyMhA+AxLlvZrn50bER6rMKC6zeEBtracX3q15H9uC7Pvi4BAPBWJGQAPGZ1dokKymsUGuRvEzLAKeb1FxEcoPyyaq3cXux0OAAA7BMJGQCPrR2bv7nAHo9Mj1NIIKNjcE5QgL+O6plgj39g2iIAwIuRkAHwiI15ZXY0wqwdG9olxulwgCb7kZGQAQC8FwkZAI+Mjs3bVD86ZpIxRsfgDcbuSsjmbspXZU2d0+EAALBXJGQADltWYYV2FNfvOzZ8V7lxwGm9kiKVGh2q6lqX5m3KdzocAAD2ioQMwGGbv2t0bGCnaEWw7xi8hJ+fH9MWAQBej4QMwGHJKanU5vxy+Uka1S3O6XCAvU5bnElCBgDwUiRkAA7Lgl2jY31TohQTFuR0OEAzx+7aINqUvs8rrXI6HAAA9kBCBqDFSiprtDa31B4zOgZvlBgZogGdohs3iQYAwNuQkAFosaVbi+R2S11iw5QUFeJ0OMB+py2yjgwA4I1IyAC0SG2dSyu2FdvjYVRWhBc7bte0RbNBtNmiAQAAb0JCBqBF1mSXqqKmTpEhgeqZGOF0OMA+HdE9XsGB/tpeVKn1uWVOhwMAQDMkZAAOmRllWLK1sHEjaH9/U2MR8E5hwQEavWuNI+vIAADehoQMwCEzIw05JVV2I+jBaTFOhwMcUMN+ZJS/BwB4GxIyAIesYXSsX0qUHX0AvN3Y3kn2+qcNO1VT53I6HAAAGpGQATgkpVW1WpdTX+p+WFdGx+AbBqVFKy48yL5+l2yp/0ABAABvEOh0AAB8y4ptRXK5pU4xoUqOCnU6HLRzGRkZHmtrQEKAZpXX6J3vlynZv4fS09M91jYAAO02IcvKytKtt96qL774QuXl5erdu7defvlljR49urG4wD333KMXX3xRhYWFOvbYY/X888+rT58+jW3k5+fr97//vT755BP5+/vr3HPP1ZNPPqnIyMjGc5YuXaprrrlG8+bNU1JSkj3/lltuceR7BryV+XlrKHU/tDOjY2g9xfm59nrKlCkeazNy6EQlnHqd/vPVHD1z9WlalZFBUgYAcJxXJ2QFBQU2wRo3bpxNyEyitHbtWsXF1VfLMh555BE99dRTevXVV9WjRw/dddddmjRpklauXKnQ0PpP7y+88EJt375dX3/9tWpqanTppZfqN7/5jd544w37eHFxsSZOnKgJEybohRde0LJly3TZZZcpNjbWngegXmZ+uUoqaxUS6K/eyf/7QAPwtIrS+sR/8lV3qN/QUR5ps6xWmrpNCu0yUDm1buXl5ZGQAQAc59UJ2cMPP6yuXbvaEbEGJulq+mn9E088oTvvvFNnnnmmve8///mPUlJS9OGHH+r888+3012mTp1qR74aRtWefvppnXbaafrb3/6mtLQ0vf7666qurtZLL72k4OBgDRo0SIsXL9bjjz9OQgY0sXzX6Fj/1CgFBrAEFa0vIa2buvQZ5LH2ZhdsUlFFjULTh3isTQAA2m1C9vHHH9vRrvPOO08zZsxQ586d9bvf/U5XXnmlfXzjxo3asWOHHdlqEBMTozFjxmj27Nk2ITPXZqSrIRkzzPlm6uKcOXN09tln23OOP/54m4w1MM9rEkIzStd0RK5BVVWVvTQwo2yGy+WyFyeZ5zfJqtNxwHf6xMRmfiZ23djrOeXVtdqQW1/MY1Cn6H2ety+m/db4PziY2Pfxhf+77IfZYc20b3daO8Tv+UBas+3Wbt/jbe/WH60Ve3p8mJZl1Sisx0iv/pn0Bt7+e6sjok+8C/3hfVxe1CeHEoNXJ2QbNmyw68Fuuukm/elPf7KjXNddd51NnC655BKbjBlmRKwpc7vhMXOdnJzc7PHAwEDFx8c3O6fpyFvTNs1je0vIHnroId1333173J+bm6vKyko5/QIoKipq/kYVjvL2PjGv2VGjRik+VAqvq0+6drdsW1l9MY+oIHULr5Hqag66fdOuad9MOfb0z8eWLVsOGPveuRXirpTs78t9b2ydGhdh20+JDj7E9g+sNdtu7fY933bz/mit2PvE+mtZlhTb7yj7WszJyfFY2+2Nt//e6ojoE+9Cf3gflxf1SUlJSftIyMx/qhnZ+stf/mJvjxgxQsuXL7frvExC5qTbb7/dJopNR8jM9Eqzzi06Otrx/zc/Pz8bi9MvRvhGn5jiOQsWLNDYSik8YM+1YeYX26IdO+3xgLRYle/lnP3ZvC3Xtm/Wc7aWkXmlCu9yCHHZ0Rip3D9S8tt3QrajoMzGPvD0aqUe4vd9IK3Zdmu37/G2d+uP1oo9OT7MrFBWXUSiSl1Be3xgB9/5vdUR0Sfehf7wPi4v6pOGWhY+n5B16tRJAwcObHbfgAED9N5779nj1NRUe52dnW3PbWBuDx8+vPGc3T8Bra2ttZUXG77eXJuvaarhdsM5uwsJCbGX3ZnOd/oFYJgXo7fEAu/vExNb49D6XpKTbUWVKiivUaC/n/qmRu03gdmb8tJi274nCzQ0yJg7Q1+8+qQqzMjbIcZlz2+47IO7YQpEw/ke1Jptt3b7rdJ2k/5ordhDggMVH+xWfrWfluZUa5IX/jx6E2/+vdVR0Sfehf7wPn5e0ieH8vxenZCZCourV69udt+aNWvUrVs3e2ymGZqEadq0aY0JmBmpMmvDrr76anv76KOPtuXwzSetZvqLMX36dPuH3qw1azjnjjvusBUYg4KC7H2mImO/fv32Ol0R6GhWZBXZ674pUQoJDPCaAg1GduZ6j7aH9i851CRk0pId/1sHDACAU7w6nb/xxhv1008/2SmL69ats2Xq//nPf9r9whoy4BtuuEEPPPCALQBiytVffPHFtnLiWWed1Tiidsopp9hCIHPnztWPP/6oa6+91hb8MOcZv/rVr+y6tMsvv1wrVqzQ22+/bfcpazolEeioqmtdWptTv4ZncGdnp+MCnpASWj8avCS7SnVmYSQAAA7y6hGyI444Qh988IFdr3X//ffbETFT5r7pOhSzeXNZWZktT29Gwo477jhb5r7pvE1T1t4kYePHj2/cGNrsXda0MuNXX31lEz0zipaYmKi7776bkveApHU5pap1uRUbHqTU6IOfDw14q/gQt1xVZSpRhJZnFWlY11inQwIAdGBenZAZp59+ur3sixklM8maueyLqajYsAn0vgwdOlQzZ848rFiB9mjVjvotHQakRtufN8DX+ftJlZuWKLzfMfp+TS4JGQDAUV49ZRGAs0oqa7SloMIe9zPFPIB2omLjAns9Y02u06EAADo4EjIA+7Q6u34PjbSYUMWE1Re8AdqDio0L7fWiLYUqrjz4PfUAAPA0EjIA+7RqR31C1r8TxTzQvtQV56pzVIAt6jFrXZ7T4QAAOjASMgB7lVtSpZ2l1Qrw81OfZM9vWgw4bXhq/V6STFsEADiJhAzAfot5dE8MV2hQy/ceA7zViF0J2fdr8uR2U/4eAOAMEjIAe3C53Vq9a7riAKYrop0alBSi4EB/ZRVWaH1umdPhAAA6KBIyAHvYkl+usuo6hQb6q3tChNPhAK0iJNBPY3rE22OmLQIAnEJCBmCfxTz6pEQpwGzaBLRTx/dJstdmPzIAAHwmIevZs6d27ty5x/2FhYX2MQC+q6bOpfW5pfa4P3uPoZ07vm99QvbThp2qrKlzOhwAQAfUooRs06ZNqqvb8w9XVVWVsrKyPBEXAIeYZKymzm33HesUE+p0OECr6psSqdToUFXVujRnY77T4QAAOqDAQzn5448/bjz+8ssvFRMT03jbJGjTpk1T9+7dPRshgDa1anv9dMV+qVHy82O6Ito38xo/sV+S3pq3Rd+uytEJu0bMAADwyoTsrLPOavwDdskllzR7LCgoyCZjjz32mGcjBNBmKuukzPxye8x0RXQU4/on24Rs+qoc3XPGQD6IAAB4b0LmcrnsdY8ePTRv3jwlJia2VlwAHLClzF9mNyYzhSsuPNjpcIA2cVzvRAUH+NsPIzbklalXEhuhAwC8fA3Zxo0bScaAdiizrP5XAqNj6EgiQgI1pmd9+XszbREAAK8dIWvKrBczl5ycnMaRswYvvfSSJ2ID0IaCEtNVWOMvU+W+bwoJGTqWE/sla+baPDtt8YqxVAsGAHj5CNl9992niRMn2oQsLy9PBQUFzS4AfE/EwHH22mwEHRYc4HQ4QJs6qX+yvZ63KV8llTVOhwMA6EBaNEL2wgsv6JVXXtFFF13k+YgAtDmX262IQSfYY6YroiPqkRhhLxvzyvTjujydMriT0yEBADqIFo2QVVdX65hjjvF8NAAcsTK3WoHRyQryc9s3pUBHZMrfG2baIgAAXp2QXXHFFXrjjTc8Hw0AR8zYXGGvO4e7FBjQol8LQLuZtvjt6ly53abeKAAAXjplsbKyUv/85z/1zTffaOjQoXYPsqYef/xxT8UHoJVV1tRp9tZKe5we0bxAD9CRHNkjXuHBAcotqdKKbcUa3DnG6ZAAAB1AixKypUuXavjw4fZ4+fLlzR5jQ03At3yTka3yGrdqi3KU2DXW6XAAx4QEBtg9yb5amW2nLZKQAQC8NiH79ttvPR8JAEd8sDDLXpet+FZ+Q852OhzAUeP6J9uEbNqqHF03vo/T4QAAOgAWiwAd2M7SKs1Yk2uPS1fyQQvQsI5syZZC5RTXT+UFAMDrRsjGjRu336mJ06dPP5yYALSRT5duV63LrV5xQdq8c6vT4QCOS4kO1bCusTYhM6NkFxyZ7nRIAIB2rkUjZGb92LBhwxovAwcOtKXwFy5cqCFDhng+SgCt4v1F9dMVT+gW5nQogNeYODDFXn+9MtvpUAAAHUCLRsj+/ve/7/X+e++9V6WlpYcbE4A2sD631I4CBPj76bj0UKfDAbzGhAEpevTL1fphXZ7KqmoVEdKiP5UAALT9GrIpU6bopZde8mSTAFrJh7tGx8b2SVRsaIDT4QBeo29KpNLjw1Vd69LMtXlOhwMAaOc8mpDNnj1boaF80g54O7Pp7Qe7ErKzR3R2OhzAq5g10iczbREA0EZaNA/jnHPO2ePN3fbt2zV//nzdddddnooNQCuZv7lAWwsqFBEcoIkDU5WxnDedwO7TFv/9w0ZNX5WtOpfbTu0FAMBrErKYmOabZfr7+6tfv366//77NXHiRE/FBqCVvL9r77FTBndSWDDTFYHdHdE9TjFhQSoor9GCzQU6ske80yEBANqpFiVkL7/8sucjAdAmqmrr9NnSbfb4nJFMVwT2JjDA3+5JZqb2fpORTUIGAPDONWQLFizQa6+9Zi+LFi3yXFQAWs23q3JUXFmr1OhQHdUzwelwAK/VdB2ZmZoPAIDXjJDl5OTo/PPP13fffafY2Fh7X2Fhod0w+q233lJSUpKn4wTg4emKZw5PY10MsB/H901ScIC/NuaV2W0ieidHOR0SAKAdatEI2e9//3uVlJRoxYoVys/Pt5fly5eruLhY1113neejBOARheXV+nZ1jj0+m+mKwH5FhgTqmN71o8hfrqDwDQDAixKyqVOn6rnnntOAAQMa7xs4cKCeffZZffHFF56MD4AHfbp0u2rq3BrQKVr9U6OdDgfweqcMSrXXXyzf7nQoAIB2qkUJmcvlUlBQ0B73m/vMYwC8U8PeY+ew9xhw0OvIzMze5VnF2pJf7nQ4AIB2qEUJ2UknnaTrr79e27bVV2ozsrKydOONN2r8+PGejA+Ah2zeWWbLd5s3lz8bnuZ0OIBPSIgM0Zge9dMWpy7f4XQ4AIB2qEUJ2TPPPGPXi3Xv3l29evWylx49etj7nn76ac9HCeCw/XfBVnt9XJ8kpUSHOh0O4DNOHcK0RQCAl1VZ7Nq1qxYuXKhvvvlGq1atsveZ9WQTJkzwdHwAPKDO5W5MyH4xuovT4QA+ZeLAVN390QotzCzUjqJKpcbwgQYAwKERsunTp9viHWYkzM/PTyeffLKtuGguRxxxhAYNGqSZM2d6MDwAnvDjujxtL6pUTFiQJgyo31sJwMExCdjI9PotXr5aybRFAICDCdkTTzyhK6+8UtHRe1Zni4mJ0VVXXaXHH3/ck/EB8IB35m9p3HssNCjA6XAAn3Pq4E72+otlJGQAAAcTsiVLluiUU07Z5+MTJ07UggULPBEXAA8pKq/RVyvr91D6xeiuTocD+KRTBtevI5uzcad2llY5HQ4AoKOuIcvOzt5rufvGxgIDlZub64m4AHjIx0uyVF3rUv/UKA1KY+8xoCW6xodrcOdoW/7+65XZOv/I9P2en5mZqby8vFaJJTExUenp+39+AEA7Tcg6d+6s5cuXq3fv3nt9fOnSperUqX5aBwDv8M78hmIeXe3aTwAt3yTaJGRfLN+x34TMJGP9BwxQRXnr7FsWFh6uVRkZJGUA0BETstNOO0133XWXnbYYGtq8ylRFRYXuuecenX766Z6OEUALZWwv1rKsIgUF+OksNoMGDsupQzrpb1+tsUVyCsqqFRcRvNfzzMiYScYuvPVRpaT38mgM2Znr9frDN9vnICEDgA6YkN155516//331bdvX1177bXq16+fvd+Uvn/22WdVV1enO+64o7ViBXCI3t01Oja+f4ri9/HmEcDB6ZUUqYGdorVye7GmrtihCw4wbdEkY136DGqz+AAAvumQErKUlBTNmjVLV199tW6//Xa53W57v5kGNWnSJJuUmXMAOK+ypk7vLaxPyH55BMU8AE84Y1iaTcg+WbLtgAkZAACtsjF0t27d9Pnnn6ugoEDr1q2zSVmfPn0UFxd3qE0BaEWfLt2uoooadY4N0/F9k5wOB2gXTh/aSQ9PXaXZG3Yqp7hSydFsEg0AaMOy902ZBMxsBn3kkUeSjAFe6PU5m+31r8akK8CfYh6Ap6otjkiPlZkg8tmy7U6HAwDoyAkZAO+1YluRFmUW2mIe7D0GeNYZQ9PstZm2CADA4SIhA9qh1+dk2utJg1KVFBXidDhAuzJ5aCeZHSQWZhZqS37rlLYHAHQcJGRAO1NaVauPFmXZ4wvHdHM6HKDdSYkO1Zge8faYaYsAgMNFQga0Mx8sylJZdZ16JUXoqJ71bxoBeL7aosG0RQDA4SIhA9oRU/X09Z82N46OmS0pAHjeqYM72WI5K7YVa31uqdPhAAB8GAkZ0I7M31ygVTtKFBrkr3NHdnE6HKDdMhutH9c70R43TBEGAKAlSMiAduRfMzfY67OGd1ZMeJDT4QDt2jkjO9vr9xdlyeVyOx0OAMBHkZAB7cSmvDJ9tTLbHl8xtofT4QDt3sSBqYoMCdTWggrN3ZTvdDgAAB9FQga0Ey//uNFuVjuuX5J6J0c5HQ7Q7oUFB2jykE72+P2FW50OBwDgowKdDgDA4Sssr9Y78+vfEF4xtqfT4QA+ISMj47DbGBxVpbdNtcXFWTo7vVYhgX4eaRcA0HGQkAHtwBtzM1VRU6cBnaJ1TK8Ep8MBvFpxfq69njJligda81PaVS9Ksak6+dI/qmzld42PlJZSfREAcGAkZICPq6516dVZm+zxFcf1oNQ9cAAVpcX2evJVd6jf0FGH3d7KwgBlFEsDz7tJY5OvU8bcGfri1SdVWVnpgWgBAO0dCRng4z5duk3ZxVVKjgpp3KwWwIElpHVTlz6DDrudyPJqZczerNxKf8V27af4zPUeiQ8A0DFQ1APwYabU9j9m1Je6v+SY7goO5EcaaGux4cFKiwmVKXy/akf96BsAAAeLd2+AD/tq5Q6tzi5RVEigpozp5nQ4QIdl1m8aGdtLbGIGAMDBIiEDfJTb7daT09bZ418f252NoAEH9UmJVKC/n/LLq1WsMKfDAQD4EBIywEd9k5GjjO3FiggO0GXHshE04KSQwAD1S63f/2+H4pwOBwDgQ0jIAB8dHXtq2lp7fPEx3RUXEex0SECHN7hzjL3OU5T8w+qnMAIAcCAkZIAP+m51rpZlFSksKMCWugfgvJSoEFvt1C1/RQwe73Q4AAAfQUIG+OTasfrRsYuO7qaEyBCnQwJgtoj282scJYsaforcVPcAABwEEjLAB0fHFm8pVEigv64c29PpcAA00S8lSgGqU1B8ZxW6mEoMADgwEjLAh9TWufTQFxn2+OKjuykpitExwJuYvQCTVL8X2fbaCKfDAQD4ABIywIf8d8FWrckuVUxYkK4d18fpcADsRaoK7HVeXajKqmqdDgcA4OVIyAAfYd7YPfb1Gnt83fg+7DsGeKlIValq2yq55acV2+pHywAA2BcSMsBH/PP7DcotqVJ6fLguOqqb0+EA2I+SBZ/a6yVbC1XrcjkdDgDAi5GQAT4gu7jSJmTGraf0t+tUAHivslUzFexXp/LqOq3eUeJ0OAAAL8a7OsAHPP7VGlXU1GlEeqxOG5LqdDgADsRVp86BZfZwUWah3a4CAIC9ISEDvNyqHcV6d8EWe3zn5AF2ryMA3i8tsExBAX7aWVatzPxyp8MBAHgpn0rI/vrXv9o3ozfccEPjfZWVlbrmmmuUkJCgyMhInXvuucrOzm72dZmZmZo8ebLCw8OVnJysm2++WbW1zStffffddxo5cqRCQkLUu3dvvfLKK232fQH789Dnq+Ryy46MjeoW73Q4AA5SoJ9bg9LqN4pemFnodDgAAC/lMwnZvHnz9I9//ENDhw5tdv+NN96oTz75RO+++65mzJihbdu26Zxzzml8vK6uziZj1dXVmjVrll599VWbbN19992N52zcuNGeM27cOC1evNgmfFdccYW+/PLLNv0egd19vyZXM9bk2k/Zb5nU3+lwAByi4V1jZca0zQhZXmmV0+EAALyQTyRkpaWluvDCC/Xiiy8qLi6u8f6ioiL9+9//1uOPP66TTjpJo0aN0ssvv2wTr59++sme89VXX2nlypV67bXXNHz4cJ166qn685//rGeffdYmacYLL7ygHj166LHHHtOAAQN07bXX6uc//7n+/ve/O/Y9A3Uut/7yef0m0FOO6qbuiWwyC/gas2dg7+RIe7wws35/MgAAmgqUDzBTEs0I1oQJE/TAAw803r9gwQLV1NTY+xv0799f6enpmj17to466ih7PWTIEKWkpDSeM2nSJF199dVasWKFRowYYc9p2kbDOU2nRu6uqqrKXhoUF9fvNeNyuezFSeb5zQJyp+PA4fXJewu2atWOEkWFBuracb1atT9NbP7+uz6faYXiA2aEwLTv1wrtt7htc27DpTXaPwit2XZrt+/xtnfrD5+K/QDtj+waq7U5pbba4lHd4xUddnh7CJq22+J3PH9LvA994l3oD+/j8qI+OZQYvD4he+utt7Rw4UI7ZXF3O3bsUHBwsGJjY5vdb5Iv81jDOU2TsYbHGx7b3zkmyaqoqFBYWNgez/3QQw/pvvvu2+P+3Nxcu67N6ReAGT1s9iYbPtUnlTUuPTp1lT3+9REpqiktVE5p68VnXrNmhDk+VAqv8/wTpcZF2PZTooM93n7L23YrxF0p2d+Xfj4Wu/Pte77t5v3hW7Hvv/2ekVK32GBtLqzWok05OrVv/bqyljA/o6Zt8zObk5Oj1sTfEu9Dn3gX+sP7uLyoT0pKStpHQrZlyxZdf/31+vrrrxUaGipvcvvtt+umm25qvG2St65duyopKUnR0dGOvxhN8RMTi9MvRrSsT56Zvk65ZTXqHBum300YpJCggFaNLysry444j62UwgPqp1d50o6CMtv+wNOrlerh9lvcth2Nkcr9I6X9VK70yti9oH2Pt71bf/hU7AfR/hE9A7R5YZaW7qjQiO7JLR4ly6+snx1i/iaaIlWtib8l3oc+8S70h/dxeVGfHEru4tUJmfmjYz4BNNUPmxbp+P777/XMM8/YohtmHVhhYWGzUTJTZTE1tX6vJnM9d+7cZu02VGFses7ulRnNbZNY7W10zDDVGM1ld6bznX4BGObF6C2x4ND6JLekSv/YtQn0Laf0U1hIUJvE1ji03gpl9d0N0whaof3Datuc33BpjfYPoDXbbu32W6XtJv3hc7EfoP3OceHqGh+mLfkVmru5QBMGNJ+V0ZI3HG3x+52/Jd6HPvEu9If38fOSPjmU5/fqV8/48eO1bNkyW/mw4TJ69Ghb4KPhOCgoSNOmTWv8mtWrV9sy90cffbS9ba5NG02ndpgRN5NsDRw4sPGcpm00nNPQBtCWnvhmjcqq6zSsS4zOGJrmdDgAPOSoHgn2OmN7sYoqapwOBwDgJbx6hCwqKkqDBw9udl9ERITdc6zh/ssvv9xOHYyPj7dJ1u9//3ubSJmCHsbEiRNt4nXRRRfpkUcesevF7rzzTlsopGGE67e//a0dcbvlllt02WWXafr06XrnnXf02WefOfBdoyNbm12it+bVbwL9p9MGyN+fTaCB9iItNkzp8eG2BP7cjfk6eWDLR8kAAO2HV4+QHQxTmv7000+3G0Iff/zxdvrh+++/3/h4QECAPv30U3ttErUpU6bo4osv1v333994jil5b5IvMyo2bNgwW/7+X//6l620CLSlv36xypa7N2/UxvSs/zQdQPtxVM/6zd0zdhSrsLx+6xUAQMfm1SNke/Pdd9/tsWDO7ClmLvvSrVs3ff755/tt98QTT9SiRYs8FidwqGatz9O0VTkK8PfTbaeyCTTQHnWKCVO3hHBt3lmu2Rt26tTBnZwOCQDgMJ8fIQPaA1eTTaAvHJOuXkmer/wGwDsc06t+9HtNdqm2F1U4HQ4AwGEkZIAX+GhJlpZnFSsyJFDXj+/jdDgAWlFyVKgGdqrfHmXm2jy7Xw4AoOMiIQMcVllTp0enrrbHvxvXSwmRe26nAKD9jZIFBfhpe1GlHSkDAHRcPreGDGhvXvpxo7YVVSotJlSXHdtjv+eaLR3y8vI8HkNGRv10SQBtIyIkUKO7xdt1ZD+uz1OvpAgFBvAZKQB0RCRkgIN2llbp+W/X2+M/Tuqn0KCA/SZj/QcMUEV5eavFU1rKJ/VAWxmRHqtlWUUqqazVwi2FOrJ7fQVGAEDHQkIGOOipaWtVUlWrQWnROmt45/2ea0bGTDJ24a2PKiW9l0fjyJg7Q1+8+qQqKys92i6AfQsK8NexvRP05Ypszd+UrwGpUYoKDXI6LABAGyMhAxyyIbdUr8/JtMd3HMIm0CYZ69JnkEdjyc6sH6UD0Lb6pURp6dYiu5ZsxppcnT40zemQAABtjAnrgEP+8vkq1brcOql/so7pneh0OAAc4OfnZ38HmM9j1ueWaX0u04YBoKMhIQMcMGtdnr7JyLabQP/pNDaBBjqyxMgQjUyPs8ffrc5Vda3L6ZAAAG2IKYvocFqrUqGRmJio9PT0/Z5T53Lrz5/9bxPo3slRrRILAN9xZI94rckuUXFlra28eELfJKdDAgC0ERIydCitXakwLDxcqzIy9puUvbdgqzK2FysqNFA3TOjbKnEA8L0CH2bq4oeLt2nJlkL1T41SSnSo02EBANoACRk6lNasVGgKY7z+8M32OfaVkJVV1erRr+o3gb7upD6Kjwj2aAwAfFe3hAj1TYm0G0V/nZGt84/oqkB/VhYAQHtHQoYOqTUqFR6Mf3y/QbklVeqWEK6Lj+nW5s8PwLuZqYpb8iu0s7Raczbk61gK/gBAu8dHb0AbyS6p1oszN9rj20/tr5DAfW8CDaBjCg8OtFMXjQWbC7S9qMLpkAAArYyEDGgjz/2Ypapal128P2lQqtPhAPBSvZMj7Royt6SvVmarpo6qiwDQnpGQAW3ALNL/clW+/PykuyYPtHsPAcD+pi5GhASosLxGs9bvdDocAEArIiEDWpnb7dYDn6+yx+eM6KwhXWKcDgmAlwsNCtCEASn2ePGWQm3eWeZ0SACAVkJCBrSyz5ftsGtBQgP99YeTKXMP4OB0T4jQkM71H+CYqYumSisAoP0hIQNaUWVNnR76on4T6CmjU5Qaw75CAA7e8X0SlRARrPLqOn29Mltus7AMANCukJABreilHzdqa0GFUqJDdOGo+ulHAHCwAgP8dergVAX6+2lzfrnWlvBnGwDaG36zA61ka0G5np62zh7fMqmfwoIocw/g0CVEhuj4vkn2eHlhgIJT+zgdEgDAg0jIgFZy/ycrVVFTZ8vcnzU8zelwAPiwwWnR6p0UKbf8lHTmrSqpohQ+ALQXJGRAK5iWkW0X4ZtpRg+cNZgy9wAOi/kdMmFAsiIC3QqMTdXf5xSqzsWCMgBoD0jIAA+rqnXr3k9W2OPLj+uhvilRTocEoB0ICQrQ0Ym1ctVUavGOKj35zRqnQwIAeAAJGeBh768q1Zb8CnWKCdV141nrAcBzYoLdyp/6jD1+avo6fbMy2+mQAACHiYQM8KCgxHR9sKrUHt99+kBFhAQ6HRKAdqZs5Xc6tXe4Pb7xncVal1P/OwcA4JtIyAAPMcs5Ek67QbUu6aT+yTplcKrTIQFop349LFqju8WppLJWl74yV7klVU6HBABoIRIywEPWFPsrpFNfRQT56aFzhlDIA0CrCQrw0z8uGqVuCeF2ivQV/5mviuo6p8MCALQA86kAD8grrdLKovp9xk5NKVXWupXKavK42+1WZWWlsrKyWpyoZWRkeChaAO1lf7KXf32Eznl+lpZsKdQNby/ScxeOUoA/HwYBgC8hIQMOk8vl1tcrs+3+QOXr5uqxh+/XY7ud4+/vr1GjRmnBggVyuQ5v/6DSUtaLAKjXMylSL148Whe+OEdfrsjW/Z+s0L0/G8QIPQD4EBIy4DDN25yvnJIqBahO+V8+o8lX3aF+Q0ftcV58qDS2suXPkzF3hr549Uk70gYADY7oHq+//WKYrntzkV6dvVn+/n62qBBJGQD4BhIy4DBkFVRozoZ8e9xLO7ShNF8Jad3Upc+g5ie63QqvK1V4QKTZ4bVFz5Wdud4TIQNoh342LE3lVbW67f1levnHTfY+b0zKMjMzlZeX12rtJyYmKj09vdXaB4DWQEIGtJBZQD91xQ65JfVPjVLCjmKnQwLQgZ1/ZH0i4q1JmUnG+g8YoIry8lZ7jrDwcK3KyCApA+BTSMiAFjBFOr7OyFZpVa1iw4M0rl+ylu1wOioAHd3uSVleabUeOXeowoLriw45yYyMmWTswlsfVUp6L4+3b2YRvP7wzfZ5SMgA+BISMqAFFm0p1Ma8MlvN7LTBnRQcyA4SALwnKTO/m25/f5k+WbJNm/LKbOGP1JhQeQOTjO0xrRsAOjDeRQKHaFthhX5cV78GYmyfRCVFhTgdEgA0c97ornr9ijGKjwjWsqwinfHMD1qwucDpsAAAe0FCBhyCksoafbZsu1xuqU9ypIZ2jnE6JADYqzE9E/TRNceqX0qUckuqdN4Ls/TQFxmqrGEDaQDwJiRkwEGqrXPZZKy8uk4JkcGaMCDFaxbLA8DedI0P13u/O0ZnDk+zHyT9Y8YGnfbUTM3fVF8dFgDgPBIy4CCLeExfnaPs4iqFBvrrjKFprBsD4BMiQwL15Pkj9M+LRik5KkQbcst03j9m6/q3Ftn1ZQAAZ/GOEjgIi7cUKmN7id1C7NQhnRQTFuR0SABwSCYOStXXN56g80Z1MVsj6qPF2zTh8Rm2+IdZGwsAcAYJGXAApprizLW7inj0TlR6fLjTIQFAi8SEB+nR84bp098fpxP7JanW5dabczN1/CPf2hGzpVsLnQ4RADocEjJgP/JKq/TF8u128+dBadEa3jXW6ZAA4LAN7hyjVy49Uu/+9mgd1TPeJmZmxOxnz/xoi39MXb5ddWbRGQCg1bEPGbAPZVW1+njJNtXUudUlLsxu/kwRDwDtyRHd4/XWb47W8qwivfTDRn2ydJvmbSqwF/N779whCbrsxDjFhLO9BwC0FkbIgH1UVPx06XaVVNYqNixIk4d0shutAkB7HTF7/JfD9cOtJ+nacb0VFx6krQUVevL7rTr24W91/ycrtSW/3OkwAaBdIiED9lJR8ZuMHO0orlRIoL9+NjxNoUEBTocFAK0uJTpUf5zUT7NuG68Hzxqk7vGhKq2q00s/btQJj36r3/7fAtaZAYCHMWUR2M38zQVanV0iMyBmRsbiwoOdDgkA2lRYcIAuODJdJ3UL0eoif700a7O+X5OrqSt22Mv4/sm6YUJfDekS43SoAODzSMiAJtbnlmrW+p32+IS+SXZTVQDoqMy62eP7JunE/ilam12i52es14eLsjRtVY69nDwwRbed2l+9kiKdDhUAfBZTFoFdckuq9OWKHfZ4WJcYDe1CRUUAaNAnJUqP/2K4pv3hRJ0zorOdRfD1ymxN+vv3uvfjFSooq3Y6RADwSSRkgKTy6lpbXcxUVOwaH6bj+yQ5HRIAeKUeiRG2AMhXN56gCQOSbcn8V2ZtsmvMXvlxI+XyAeAQkZChw6t1Na+oeNrgTvKnoiIA7Ffv5Ej965Ij9PoVY9Q/NUrFlbW695OVOvu5H20ZfQDAwSEhgzp6RcXpq3K0vahSwaai4jAqKgLAoTi2d6I+u26s/nzWYEWFBmrp1iL97Jkf9OdPV6qius7p8ADA65GQoUNblFmojO0lMvs9nzY4VXERVFQEgENl9mm86KhumnbTCTp9aCeZWYv//mGjTntqphZsLnA6PADwaiRk6LA27yzTzHV59tisGeuWEOF0SADg05KjQ/XMr0bq5UuPUGp0qDbmlem8F2bp4amr7BpdAMCeSMjQIZXXyu6lYwxKi7ZVFQEAnjGuX7K+vOF4W43RjJY9/9163TYtT4FxaU6HBgBeh4QMHY9/gObkBaqyxqXkqBCd2DfJ7rUDAPCcmPAgW43xhSmjFB8RrI2Ftep0yRPKLOOtBwA0xW9FdDhxJ16q/Gp/W8TjtCGdFBjAjwEAtJZTBqfq8+vGamBSsPxDwjVvZ6C+ychWbZ3L6dAAwCvwThQdyk9bKxR9xFn2eOLAFMWEBTkdEgC0e6kxobrvhHgV/viGqW+rFduK9d+FW1VaVet0aADguECnAwDayqa8Mj0zr35vnD5RdeqVFOl0SADQIhkZGa3WdmJiotLT01ulEmPRD2/o9LPP0/z8EGUXV+mtuZk6fWiaTdgAoKMiIUOHUFlTp6tfX6jyGrcqt6zQ4GP6OB0SAByy4vxcez1lypRWe46w8HCtyshQly5dWqX9lFC3zj8yXZ8s2aadZdV2pGzCgGT1T41ulecDAG9HQoYO4Z6PVihje7GiQ/y19eNH5H/si06HBACHrKK02F5PvuoO9Rs6yuPtZ2eu1+sP36y8vLxWS8gMM138F6O76ssVO7Qhr0xfrshWWVWdRnWLa7XnBABvRUKGdu+/C7bq7flb7ObPNx4Vq8tKdzodEgAcloS0burSZ5B8mSmsZDaR/mFdnhZmFtrr8upaHdc7kcq3ADoUEjK0a6t3lOjOD5fZ4xvG99Ww+BKnQwIAn1ij5na7VVlZqaysLI8kSHtb92baHdsnSeHBgY2JWVl1nU4ekGLXnAFAR0BChnarorpOv39zod1vbGyfRP3+pN5avHiR02EBgE+sUfP399eoUaO0YMECuVyeK1FfWlq6x31mqmJ4cIAth28+SKurc9ty+SRlADoCEjK0Ww98tlJrskuVGBmiv/9yuPz5ww4Ah7RGLT5UGlvpmbYz5s7QF68+aUfd9mZAp2iFBgXos6XbtS63VFOX7yApA9AhkJChXZq6fLten5Npj//+y2E2KQMAHMIatd4DFV5XqvCASDO30CMFQw6kR2KEJg/tRFIGoENhY2i0O1mFFbrlv0vt8VUn9LTrEwAAvqEhKQvw87NJmanE6HK7nQ4LAFoNCRnaldo6l258a7GKK2s1rGus/jixn9MhAQAOIylbm1Oqb1fl2CIjANAekZChXXnm23WauylfkSGBeur84QoK4CUOAL6alE0anCIzWXH5tmLN3sCWJQDaJ96tot2YuzFfT01ba48fPHuwuiVEOB0SAOAw9EmO0kn9k+3xvE0FWphZ4HRIAOBxJGRoFwrLq3XDW4vkckvnjuyiM4d3djokAIAHDO4co2N6JdjjmWvztGpHfSVIAGgvSMjg88y6glvfW6ptRZV2isv9Zw5yOiQAgAeN7hanEV1j7fHXK7O1Jb/c6ZAAwGNIyODz3py7RV+uyFZQgJ+evmCEIkLYzQEA2hM/Pz+N7ZOo3smRdibEp8u2a2dpldNhAYBHkJDBp23ILdWfP11pj289pb+d2gIAaJ9J2aSBKeoUE6rqWpc+WrJNZVW1TocFAIeNhAw+q8aUuH9niSpq6nRs7wRddmwPp0MCALSiwAB/nTEsTbFhQSqprLVJmUnOAMCXkZDBZz0zfZ2WbClUdGig/nbeMPn7m+LIAID2LCwoQGcOT7PXuSVV+nz5drnMPEYA8FEkZPBJpvSx2XPMeODsIeoUE+Z0SACANhIbHqyfDUtToL+fNu8s17erzcbRTkcFAC1DQgafY9YM3Pj2YtW53PZTUvNHGQDQsaTGhOqUwan22GwcvbqYtzQAfJNX//Z66KGHdMQRRygqKkrJyck666yztHr16mbnVFZW6pprrlFCQoIiIyN17rnnKjs7u9k5mZmZmjx5ssLDw207N998s2prmy8E/u677zRy5EiFhISod+/eeuWVV9rke8She+CzlfYT0bSYUN1/5mCnwwEAOKRXUqRO7Jtkj1cUBSpi0DinQwKA9pWQzZgxwyZbP/30k77++mvV1NRo4sSJKisrazznxhtv1CeffKJ3333Xnr9t2zadc845jY/X1dXZZKy6ulqzZs3Sq6++apOtu+++u/GcjRs32nPGjRunxYsX64YbbtAVV1yhL7/8ss2/Z+yf2X/GlLn385P+9othigkLcjokAICDhnWN1cj0+j3KEk67QfO3VTodEgAcEq/esGnq1KnNbptEyoxwLViwQMcff7yKior073//W2+88YZOOukke87LL7+sAQMG2CTuqKOO0ldffaWVK1fqm2++UUpKioYPH64///nPuvXWW3XvvfcqODhYL7zwgnr06KHHHnvMtmG+/ocfftDf//53TZo0yZHvHXsyi7dve2+pPb7iuB46plei0yEBALzAcb0TlZe3U5nlAfrb7AINH5SvI3vEOx0WAPh+QrY7k4AZ8fH1v2RNYmZGzSZMmNB4Tv/+/ZWenq7Zs2fbhMxcDxkyxCZjDUySdfXVV2vFihUaMWKEPadpGw3nmJGyfamqqrKXBsXFxfba5XLZi5PM87vdbsfj8CTz/dz63hLtLKtWv9Qo3XRynxZ9f6Ydf/9dA8MeXgFuajyatv321ra53XBpjfYPU2u23drtt7jtg+wTr4zdC9r3eNu79YdPxd6G7bd57If5e2u/bXuQaXNUQp1WL10g9T5Sl786T29dOUYDOkWrPWmPf999Gf3hfVxe1CeHEoPPJGTmmzIJ0rHHHqvBg+vXDe3YscOOcMXG1k9VaGCSL/NYwzlNk7GGxxse2985JsmqqKhQWFjYXte33XfffXvcn5uba9e1Of1/ZZLXZsmHj/toeZ6mr8pVUICf7prQVUX5O1vUjumbUaNGKT5UCq8r9WiMqXERtu2U6OC9tO1WiLtSsj+bfq3Q/uFpzbZbu/2Wt31wfeKdsTvfvufbbt4fvhV727XfprG7Sg/791Zbxp4YJqVv/VrxRx6t1fm1mvLvOXrmnL7qZR5oJ9rj33dfRn94H5cX9UlJSUn7S8jMWrLly5fbqYTe4Pbbb9dNN93UeNskb127dlVSUpKio6MdfzH6+fnZWJx+MXpCdnGlnvlhiT3+48S+OmZgtxa3lZWVZUdWx1ZK4QGRHoxS2lFQZtseeHq1Undv237KLJX7R8ougPN0+4epNdtu7fZb3PZB9olXxu4F7Xu87d36w6dib8P22zR20xeH+XurLWPPr5QWzpujb/+eoIfnVWrFtmJd+/5a/eeyIzQoLUbtQXv7++7r6A/v4/KiPgkNDW1fCdm1116rTz/9VN9//726dOnSeH9qaqot1lFYWNhslMxUWTSPNZwzd+7cZu01VGFses7ulRnNbZNY7W10zDDVGM1ld6bznX4BGObF6C2xHK77PslQSWWthnWJ0RVjex3WBtDm/6VxCNkDbzCacjcMle+rbXNfw6U12j8Mrdl2a7d/WG0fRJ94bewOt98qbTfpD5+LvY3ab/PYD/P31n7bbgWm/ciQAL1xxVG6+KU5WrK1SBf+a65evexIjUiPU3vQnv6+twf0h/fx85I+OZTn9+pXjxluNMnYBx98oOnTp9vCG02ZqQ9BQUGaNm1a432mLL4pc3/00Ufb2+Z62bJlysnJaTzHVGw0ydbAgQMbz2naRsM5DW3AOVOXb9fUFTvs5p9/PXeoAg4jGQMAdAwx4UF67YoxGt0tTsWVtbro33M1a32e02EBgO8lZGaa4muvvWarKJq9yMxaL3Mx67qMmJgYXX755Xbq4LfffmunQlx66aU2kTIFPQxTJt8kXhdddJGWLFliS9nfeeedtu2GEa7f/va32rBhg2655RatWrVKzz33nN555x1bUh/OKaqo0d0frbDHvz2hV7tbnA0AaD1RoUF2ZOyonvEqrarVxf+eq//7abPTYQGAbyVkzz//vF2Yd+KJJ6pTp06Nl7fffrvxHFOa/vTTT7cbQptS+Gb64fvvv9/4eEBAgJ3uaK5NojZlyhRdfPHFuv/++xvPMSNvn332mR0VGzZsmC1//69//YuS9w776xerlFNSpZ6JEbr2pN5OhwMA8DERIYF65dIj9bNhaap1uXXXh8t1xwfLVFPnfAU2APCJNWRmyuLBLJh79tln7WVfunXrps8//3y/7Zikb9GiRS2KE563MLNAb87NtMcPnTNEoUEBTocEAPBB5u/Hk+cPV/9OUXr0y9V6fU6m1maX6vFfDlOXuHCnwwMA7x4hQ8fkcrl138f1UxXPG9VFY3omOB0SAMDHF/n/7sTeevGi0YoIDtDcTfk65YmZemtu5kF9+AsArYmEDF7nvYVbbWWsyJBA3XxKP6fDAQC0ExMGpujT68ZqVLc4u67stveX6dJX5ml7Uf3adABwAgkZvEpJZY0enrraHv/+pN5Kjjr4PRwAADiQHokReueqo/Wn0/orONBf363O1Ul/m6Fnpq9VZU2d0+EB6IC8eg0ZOp5nvl2nvNIqdYoM0PDwQi1cuNCj7WdkZHi0PQCA7zFbqPzm+F4a1y9Zt7+/TPM3F+hvX63R2/O36M7JAzVxYIqd5ggAbYGEDF5jY16Z/j1zoz1e8srdOuquea32XKWlpa3WNgDAN/RJidK7vz1aHy/Zpr98nqEt+RW66v8W6LjeibrnjIH2cQBobSRk8BoPf7HKliWu2DBfZ//8F0pNv93jz5Exd4a+ePVJVVZWerxtAIDvMSNhZw7vrAkDUvTcd+v04vcb9cO6PJ3y5ExdcnR3XT+hj2LCgpwOE0A7RkIGr7Bsa5GmrtghM0Gk4NuXlHriE+rSZ5DHnyc7c73H2wQAtI89y26e1F+/GN1VD3yWoa9XZuulHzfqo8VZ+uOkfvZ+M9URADyNoh7wCn/7qr6Qx/HdwlSTV7//GAAAba1bQoRevHi0/nPZkeqVFKGdZdV2ndmZz/6g+ZvynQ4PQDtEQgbHzduUrxlrchXo76dfDop0OhwAAHR83yRNveF43XX6QEWFBGp5VrF+/sJsPfDpSlXVUo0RgOeQkMFRZkPOR7+sHx07b3RXpUYyixYA4B2CAvx1+XE99O3NJ+oXo7vY+/71w0ad/ewsrcspcTo8AO0ECRkcNXNtnuZuzLd7wVw3vrfT4QAAsIfEyBA98vNhdipjfESwVm4v1ulP/6C35zHFHsDhIyGDo6NjDWvHpozppk4xYU6HBADAPp08MEVTrx+rsX0SVVnj0q3vLdM9Hy1XbZ3L6dAA+DASMjjmuzW5Wrq1SGFBAbr6xF5OhwMAwAElR4fq1UuP1B9O7mtvvzp7sy55ea4Ky6udDg2AjyIhg2Oe/66+BP2Uo9KVFBXidDgAABwUf38//X58H70wZZTCgwP047qdOuvZH7Uht9Tp0AD4IBIyOGLB5ny7diwowE+XH9fT6XAAADhkpwxO1XtXH6POsWHatLNc5z4/y/59A4BDQUk7OOL57zbY63NGdFFqTKjT4QAA2omMjIxWaTcxMVHp6el73D+gU7Q+uvZYXf7KPC3ZWqRfvThHT54/wiZrAHAwSMjQ5tZkl+ibjGz5+Um/OYHRMQDA4SvOz7XXU6ZMaZX2w8LDtSojY69JmanC+OZvjtLv31ikaatydPXrC3TP6QP162N7tEosANoXEjK0uRdm1K8dO2VQqnolsRE0AODwVZQW2+vJV92hfkNHebTt7Mz1ev3hm5WXl7fXhMwIDw7UPy4apbs/XqE35mTq3k9WaltRpW47pb9dcwYA+0JChja1taBcHy/eZo+prAgA8LSEtG7q0meQI88dGOCvB88abNeUPfrlav3z+w3aVlihx34xTCGBAfv92szMTJvwHWi7mMrKSmVlZcnPTDM5zOmWALwDCRna1L9mblSty63jeidqaJdYp8MBAMCjTKJ0zbje6hQTqlv+u1SfLt2unJIqvXjRaMWEB+0zGes/YIAqysv327a/v79GjRqlBQsWyOVyeWS6JQDnkZChzRRX1ujd+Vvs8VWsHQMAtGPnjOyilOhQ/fb/Ftiqwue+MEuvXHqEusSF73GuGRkzydiFtz6qlPT9zx6JD5XGVnp2uiUAZ5GQoc28M2+Lyqrr1Dcl0o6QAQDQnis4hkm674RYPTAzX+tySnXGkzN059h49YgL2mu7Jhnb73RLt1vhdaUKD4g0Q3Et+yYAeB0SMrSJOpdbr87eZI9/fUyPQ5r7DgCAL1dwDIhKVPJ596ogqbtu/DRTuR/9VZUbF+5xXmkpG0sDHREJGdrEtIxsbcmvUGx4kM4e0dnpcAAAaNMKjtUu6adcl3IVrpRf3KeBMXXqH+2yA10Zc2foi1eftAU7AHQ8JGRoEy//WD86dv4R6QoL3n+lKQAA2mMFx/Q+Ln27KlcrtxdrZVGgSgPCNWlQiuIz67eDAdAx+TsdANq/jO3Fmr1hpwL8/XTx0d2cDgcAAEcE+vvr5IEpOnlAigL9/ZSZX6435maqQBFOhwbAQYyQodW9smt0zGwEnRZrljgDANBxDUyLVkp0iD5ftkP55dVaoXQlnHq9atysrwY6IkbI0Kryy6r14eIse3zpsd2dDgcAAK+QEBmi84/sqmFdYkz5REUOPVnzK5JtNUazATSAjoOEDK3qzbmZqqp1aXDnaI3qFud0OAAAeI2gAH+d2C9ZQ7VZNTu3qFoB+mzZdr2/KEu5JVVOhwegjZCQodXU1Ln0f7M32+NLKXUPAMBeRatC216+TumBJXa99daCCru27JuMbJVV1TodHoBWRkKGVjN1+Q7tKK5UYmSITh/WyelwAADwXnU16hFcoouP6qa+yZH2rhXbivXKrE36YW2eKqrrnI4QQCuhqAdazcs/brTXF45JV0ggpe4BADiQ6LAgnTqkk4YVVuiHdXnaXlSpBZkFWppVqBFdY3Vs5yCJP6lAu0JChlaxZEuhFmYWKijATxcele50OAAA+BRTlfi8UV20aWe53TrGrCmbu6lAS7f6aUR6rYZ3jVNwIBOdgPaAhAwtkpmZqby8vH0+/sRPBfb6mC6h2rp2pbYeZLsZGRkeihAAAN9m1l73SIxQ94Rwrc8ts4mZqV48e0O+Fm8p0ujucRraOUaBASRmgC8jIUOLkrH+Awaoorx8r48HRMSp89UvyS8gSG/d9xv9J3v9IT9HaWmpByIFAKB9JGa9kyPVMzFcm7fnacbmchVV1Gjm2jwtzCzQkd3jNSgtxhYEAeB7SMhwyMzImEnGLrz1UaWk99rj8RWFAVpVHKCEEJfOvfvRQ2o7Y+4MffHqk6qsrPRgxAAA+D5/Pz8NSglTemqiMrJLNHdjvkoqa/Xt6lwt2FygI3vEa0BqtPxJzACfQkKGFjPJWJc+g5rdV1vn0uc/bjLlojSmT5q6pEQdUpvZmYc+mgYAQEdiRsIGp8Wof2qUVmQVa+6mfBVX1uqbjBzN31ygo3okqG9KJNvNAD6ChAwetSa7VBU1dYoMCVSvpPqyvQAAwPMC/f01rGusBqZFa9nWIpuMFZbXaOqKHZq3KVhH90pQsNvpKAEcCAkZPMbtdmvxlkJ7PKxLDFMmAABoA0EB/hrZLU6DO8fYv8OmTP7Osmp9unS74oIDFdpjpP0bDcA7UZYHHrOtsFK5pVUK9PfToM4xTocDAECHYsrgm3Vklx7TXUd0j7NbzxRU+yvlF/frzm936qcNO50OEcBekJDBYxZtqS91b+a0hwWxayUAAE4IDQrQMb0S9etjuqtPVJ1cNVXKyKvR+f/8SVP+NcdWZgTgPUjI4BHFFTXakFtmj818dgAA4Kzw4EANjavTtn9eqVN7h9sRsx/W5emc52bp8lfmacW2IqdDBEBCBk9ZsrVQZnZ617gwJUaGOB0OAADYpa40X1eOjNG3fzxRvxzd1VZpnLYqR5Of+kG/e32BVm4rdjpEoEMjIcNhq6lzacWuX+bDGR0DAMArdYkL18M/H6qvbzxeZw5Pk6mK//myHTrtqZm6+KW5mr1+J8U/AAeQkOGwmWSsqtalmLAg9UiMcDocAACwHz2TIvXk+SM09frj9bNhaTJFkb9fk6sLXvxJZz03S1OX75DLRWIGtBUSMhyWOpdbCzbXLw4emR7LJpQAAPiIfqlReuqCEfruj+N00VHdFBLoryVbCvXb1xZowt9n6O15maqsqXM6TKDdIyHDYVm9o0SlVbUKDw7QwE7RTocDAAAOUXpCuP581mD9eNtJunZcb0WHBtpCXbe+t0xHPTRND362Upvy6gt3AfA8EjK0mJlmPn9zvj0emR6nwABeTgAA+CpTlOuPk/pp1u3jdcdpA9Q5NkyF5TV6ceZGnfi373TRv+fY6Yy1dS6nQwXalUCnA4DvyqrwU0F5jZ3iMISNoAEAaBciQwJ15fE9ddlxPfTtqhy9NmezZqzJ1cy1efaSGh2q84/sqnNHdlHX+HCnwwV8HgkZWmx1UUDjvmPBgYyOAQDQnpjy+BMGptjLlvxyvT4nU+/O36IdxZV64pu19jKqW5wtDDJ5aCe2vQFaiIQMLRLaY6QKa/ztJpOUugcAwLtlZGQcdhsTU6Rxp8Trp6xKfbOhXMtzqm1hL3O5/9OVOqZXgs4c3lmTBqUoKjTII3EDHQEJGQ6Z2aMk5phf2uPBnWMUFlQ/UgYAALxLcX6uvZ4yZYrH2w6IjFd4/+MUNfgkKaV345TGP33gb5OzE/sm6cR+yerOljjAfpGQ4ZDN21al0C6D5O/ntsU8AACAd6ooLbbXk6+6Q/2GjvJ4+9mZ6/X6wzfo0+/maH1tnD5anKX1uWX6bnWuveiTleqWEN6YnB3VM0FhwQf/QW5mZqby8vLUWhITE5Went5q7QMHg4QMh6SmzqX/W1r/y71PlMsu/AUAAN4tIa2buvQZ1Grtd4oK1OSRffT7k3prdXaJZuxKyEw15s07y/Xq7M32Ytacj0qP09G9EmxyNqxrjEICA/aZjPUfMEAV5eWtFndYeLhWZWSQlMFRvJvGIXlr3hZlldSprrxI/bqEOR0OAADwIn5+fuqfGm0vV53Qy+5VOmtdnr5bk2uTtKzCCs3esNNeDFOp2RQGObpngo7qlaBhXf5XKMyMjJlk7MJbH1VKeq9WGt272T4PCRmcREKGg2Z+qT75zRp7XPTjmwrqf5nTIQEAAC9mZtJMHJRqL2YNupnO+NOuhGzOhp3KK63WrPU77UVfS6FB/0vQ4mqrJf9Am4y15uge4DQSMhy0f8xYb39xdooM0ObFX0giIQMAAAc/etY7OdJephzVbVeCVqrZG/L10/qdNlHbWVatH9fttBej6w1vaWZOoHoF5atLbJhSokNtOX6gPSEhw0HZUVSpF2dusMcXDY3ST646p0MCAADtoKz+oGBp0ADpsv7x2lpcq+W51bak/pLt5SpXqHIqpRwzgmbeuPr7KS02TF3iwtQ1LlzJ0SHy9yNBg28jIcMBmU+w7vpouSprXHYawZjObPwIAABat6y+EZSYrvG/f1Q1ESl2/VlFTZ0y88vtRdpp16CZxCw9PlzpCeGKCWP/M/geEjIc0IeLs/T1ymy7CfQDZw1WxfZ1TocEAADaeVn9jLkz9MWrTypZRRo+dLj9gNhMacwqqNCWgnJtya9QVa1L63JL7cUwCVnX+DCboJlELZS9UuEDSMiwX9nFlbrnoxX2+PrxfTSgU7QWbnc6KgAA0N7L6psqiLuvQUuMDLGXYV1j5XK5lV1S2ThiZpZXFFXUqCirRsuzimUmMpopjSYx6xwbpk6xofsssQ84iYQM+2Q+ibr9/WUqrqzVkM4x+u0Jni85CwAA0BL+/n7qFBNmL2N6JKi61qWtheXasrPCJmj55dXKLq6yl/mbC2yClhQVYtegmQTNj+Xw8BIkZNin/y7YqumrchQc4K/HfjFMgQH1+4IAAAB4G7N/Wc/ESHsxSipr7LRGk6RtK6wfPcspqbKXxVsKzVco7Yrn9fTcQk2o2awRXePUNyWS9ztocyRk2KuV24p178f1UxVvmthXfVOinA4JAADgoEWFBmlgmrlE29ullbW2MMi2wgp7bdajBSV01bebKvTtpuX2nPDgAA3tEqMR6XEamR6n4V1j7aga0JpIyLCH7UUVuuyVeSqrrtMxvRJ05dieTocEAABwWCJDA9UvNcpejPWrVuilJ/6i3939N+2oCbWjZqVVtfrJ7Iu2Ib/x60yREDN6NiI91iZpZj29GY0DPIWEDM2YX0SXvTJfO4or7caNz08ZxQaMAACg3QkJkCo2zNevhkRp5MiRqnO5tS6nVIsyC7Qos1CLthRobU6pnfZoLh8v2Wa/ziRjZm398K4x6hntrxNDotQ5LsLpbwc+jIQMjWrrXLr2jYXK2F6sxMhgvfzrI9jPAwAAdAjmA+iGEbTzj0y39xVX1mjpliIttElagRZtKVRheY0WbC6wF+vzDUqNDrUjaPWXOJuwUXIfB4uEDFZFdZ1ueHuRvludq9Agf/3rkiPUNT7c6bAAAABaVUZGxn4fN++Gjoszl0C5hyZoe2md1uys1mpzya1WZkmdnVn0xfId9mIE+El9EsM0pneyTdBMomb2RjOl+4HdkZBBO0urdPmr8+3caVNR8ekLRtpFrAAAAO1VcX6uvZ4yZUqLvt7f31+jRo1S5tLlCkzqqZC0/grp3N9eKzJOq3IrtCp3s16dvdmeHx8RrBFdYzWoc4wGdorWoLRodYkLI0kDCVlHtyG3VJe+Mk+bd5bb6Yn/vGiUxvRMcDosAACAVlVRWmyvJ191h/oNHdWiNuJDpbGVze9zu6XMLWv1xUfv69I/3q+tlUFasa1I+WXVmrYqx14aRIUG7krOYjSgU5T6pESpZ1KEokNZMtKRkJB14E2f3563RQ9+nqGSylpbQejlXx9pC3kAAAB0FAlp3dSlz6BD/0K3W+F1pQoPiJR2G+UyN8tXzdRlI6JtwZDKmjqt3F6sJVsK7dZCK7YVa21OiX0PNmdjvr00lRgZol5JEeqZFLnrOsJOeewSF87atHaIhKydys3NVVZW1l6HwbeX1Or5BUVanlNtb/dLCNKtx0apeOsaLdx6+HOtAQAAsOd7pmFh0rBeknpFqKYuXFtLarWxoEYbC2u0ubBWWSW1Kqh0Ka+0yl52T9SM2FB/JYX5KSUySMkRAUqOCNx1HaCk8AAFmQVshykxMVHp6fWFTdD6SMjaoS1btujq3/1Os2fNksvlarzfPyxa0aPPVNQRZ8o/KFSumkoVfv9/+mrBJ/rK/b/zDlZpaamHIwcAAOjY69P8gsMUFN9FQfGdFZhQf21uB8akyD8kXIWVLhVWSmsL6vb69bUlO1VXkqfakjzV2ePc+vuKzX25qivNl1x7/9oGYeHhWpWRQVLWRkjI2qG8vDxVV1XpVzc/rOT0Xqqsk9YWB2h9qb/q3PWfmiSFuDQyzV+Rl14imcshyJg7Q1+8+qQqK3ebNA0AAACPrE/bnVmbVuOq1vIlCzV35nQNOeVXikjqqrJaP5XXyV6b93mBUQn2EqJ++2pJof5SWKBbYQFSWIDbHofvOi7L2az3Hv2jfT9JQtY2SMjaKbf85ErqraWVcdqQW6Y681MsKTkqREf2iFfPxIgWV/XJzlzv4WgBAADanxavT9uPgqz1Kl8zS/0u/LWGj+nfrD5ARU2dXZdWWlWr0spaley6trd3Hde5pUqXVFntp107qe2mt7rd/KEu/ShbXX6YafdYS44OtdepMSFKsdehSokKVWx4EFUiPYCErB36b0apdh5zvX7MNRV66qcVmh8ik4h1T2APDAAAgPbGvL8LDw60l5R9nNOQtO0tWWtI5EoqquWSn4qqXCraVYBkX0IC/esTNJu0hexK2kKbJW3mfgqR7B8JWTu0pahWrtAYBfm7NTAt1pZSTYoKcTosAAAAeEnSlryPc7asWaEnb/61/vrUPxSTmq78CpfyK+p2XVzaueu4pNqtqlqXMvPL7WV/ooL9FB8WoLhQf4UFStGhgYoM9lNksL8ig/wVYa6D/RQe5K/wID9F7LoO8D+0QQSTcAYHBys5eV/fnXciIdvNs88+q0cffVQ7duzQsGHD9PTTT+vII4+UL/lZvwgt/eI1nXTqz9Str2+9IAEAAOCckoJcuSqKdcuVF+z/xIAgBUbGKyAyQQFRuy6R8QpsuB1Zv5bNLzDYJm8l1bXaXNTwxfWVvg/EVV0pV1WZ3NXl9tpV1fzaba4rS1VXUSxXeZHclSUaMaiv3vz38+revbt8BQlZE2+//bZuuukmvfDCCxozZoyeeOIJTZo0SatXr/apTLtXXJBCc1YowO9nTocCAACADlqQpKEQSUWdnyrqpI1rV2v5glnqd+ypik5MU7VLqnb5qWbXda1LqnGrsQidf3CovUgJB/2c291u5eTlkZD5qscff1xXXnmlLr30UnvbJGafffaZXnrpJd12221OhwcAAAD4bEESv9x1mj3/Yw084/RmBUl2V+dyq7rWparaOntdXWeOXbvuczV7rNJcquvq18ZVVKmmokwBPlYvgYRsl+rqai1YsEC33357433+/v6aMGGCZs+evcf5VVVV9tKgqKh+DLawsLDZ3l9OKCkpsTFkrV2hqor9z+ltiZwtG+wc5OzNa7Q+Itxn2m7t9g/UdlyoVHAYOwXw/+75tg+mT7w1dqfbb422m/aHr8XeVu23deyH+3trf217Wnv6f9+fQ+0Tb4rdm9r2VPv76o+O/v/SWm0H77rswdSxC5Ly8jdq7cwPVXLm3+x7cicVFxc3rms7ED/3wZzVAWzbtk2dO3fWrFmzdPTRRzfef8stt2jGjBmaM2dOs/Pvvfde3XfffQ5ECgAAAMAXbNmyRV26dNnvOYyQtZAZSTPrzRqYEan8/HwlJCQ4XlbeZORdu3a1L4Do6GhHY0E9+sT70Cfehf7wPvSJ96FPvAv94X2KvahPzJiXmbWWlpZ2wHNJyHZJTExUQECAsrOzm91vbqempu5xfkhIiL00FRsbK29iXohOvxjRHH3ifegT70J/eB/6xPvQJ96F/vA+0V7SJzExMQd1nn+rR+IjzJ4Fo0aN0rRp05qNepnbTacwAgAAAICnMELWhJmCeMkll2j06NF27zFT9r6srKyx6iIAAAAAeBIJWRO//OUvlZubq7vvvttuDD18+HBNnTpVKSkp8iVmKuU999yzx5RKOIc+8T70iXehP7wPfeJ96BPvQn94nxAf7ROqLAIAAACAQ1hDBgAAAAAOISEDAAAAAIeQkAEAAACAQ0jIAAAAAMAhJGQ+7Pvvv9cZZ5xhdwD38/PThx9+2OxxU6/FVIzs1KmTwsLCNGHCBK1du9axeNu7hx56SEcccYSioqKUnJyss846S6tXr252TmVlpa655holJCQoMjJS55577h6bkcNznn/+eQ0dOrRxg0izp+AXX3zR+Dj94ay//vWv9nfXDTfc0HgffdK27r33XtsHTS/9+/dvfJz+cEZWVpamTJli/9/N3+8hQ4Zo/vz5jY/z971tde/efY+fE3MxPxsGPydtq66uTnfddZd69OhhX/+9evXSn//8Z/tz4as/IyRkPszskTZs2DA9++yze338kUce0VNPPaUXXnhBc+bMUUREhCZNmmR/ccDzZsyYYX8h//TTT/r6669VU1OjiRMn2n5qcOONN+qTTz7Ru+++a8/ftm2bzjnnHEfjbs+6dOli3/QvWLDAvpk56aSTdOaZZ2rFihX2cfrDOfPmzdM//vEPmzA3RZ+0vUGDBmn79u2Nlx9++KHxMfqj7RUUFOjYY49VUFCQ/QBp5cqVeuyxxxQXF9d4Dn/f2/73VdOfEfM33jjvvPPsNT8nbevhhx+2H7g+88wzysjIsLfNz8TTTz/tuz8jpuw9fJ/pyg8++KDxtsvlcqemprofffTRxvsKCwvdISEh7jfffNOhKDuWnJwc2y8zZsxo/P8PCgpyv/vuu43nZGRk2HNmz57tYKQdS1xcnPtf//oX/eGgkpISd58+fdxff/21+4QTTnBff/319n76pO3dc8897mHDhu31MfrDGbfeeqv7uOOO2+fj/H13nvmd1atXL9sX/Jy0vcmTJ7svu+yyZvedc8457gsvvNBnf0YYIWunNm7caDe3NkO0DWJiYjRmzBjNnj3b0dg6iqKiInsdHx9vr80ojRk1a9onZmpQeno6fdJGUxzeeustO2Jppi7SH84xI8mTJ09u9n9v0CfOMNN4zNT3nj176sILL1RmZqa9n/5wxscff6zRo0fb0Rcz/X3EiBF68cUXGx/n77uzqqur9dprr+myyy6z0xb5OWl7xxxzjKZNm6Y1a9bY20uWLLEj+6eeeqrP/owEOh0AWod5IRopKSnN7je3Gx5D63G5XHZdjJl2MnjwYHuf+X8PDg5WbGxss3Ppk9a1bNkym4CZaQpmbv8HH3yggQMHavHixfSHA0xSvHDhQjsFaHf8jLQ98wbllVdeUb9+/exUrPvuu09jx47V8uXL6Q+HbNiwwU7Huummm/SnP/3J/qxcd911ti8uueQS/r47zKzXLyws1K9//Wt7m5+TtnfbbbepuLjYJr4BAQH2A9cHH3zQfqBk+OLPCAkZ0EojAOYNTdO1GHCGeaNpki8zYvnf//7XvqExc/zR9rZs2aLrr7/err8IDQ11OhxIjZ8oG2Y9n0nQunXrpnfeeccuhIczH+iZEbK//OUv9rYZITN/T8xaGPP7C87697//bX9uzKgynPHOO+/o9ddf1xtvvGHXwJq/8eZDcNMnvvozwpTFdio1NdVe717lx9xueAyt49prr9Wnn36qb7/91haVaGD+381UB/PJWlP0Sesyn1z27t1bo0aNspUwTSGcJ598kv5wgJnak5OTo5EjRyowMNBeTHJsFl6bY/PpJX3iLPMpf9++fbVu3Tp+RhxiqsKZUfymBgwY0DiVlL/vztm8ebO++eYbXXHFFY338XPS9m6++WY7Snb++efbCqQXXXSRLaxi/sb76s8ICVk7ZUqBmhedmWPbwAzvmkozZvoWPM/UVjHJmJkSN336dNsHTZmEwFTNatonpiy++SNLn7Ttp89VVVX0hwPGjx9vp5CaTzMbLmYkwEwzaTimT5xVWlqq9evX26SAnxFnmKnuu2+ZYtbKmJFLg7/vznn55Zftuj6zBrYBPydtr7y8XP7+zVMYM3XR/H332Z8Rp6uK4PAqlS1atMheTFc+/vjj9njz5s328b/+9a/u2NhY90cffeReunSp+8wzz3T36NHDXVFR4XTo7dLVV1/tjomJcX/33Xfu7du3N17Ky8sbz/ntb3/rTk9Pd0+fPt09f/5899FHH20vaB233XabrXK5ceNG+zNgbvv5+bm/+uor+zj94bymVRYN+qRt/eEPf7C/s8zPyI8//uieMGGCOzEx0VaJNeiPtjd37lx3YGCg+8EHH3SvXbvW/frrr7vDw8Pdr732WuM5/H1ve3V1dfZnwVTB3B0/J23rkksucXfu3Nn96aef2t9d77//vv29dcstt/jszwgJmQ/79ttvbSK2+8W8UBvKft51113ulJQUW+pz/Pjx7tWrVzsddru1t74wl5dffrnxHPOL4He/+50tvW7+wJ599tk2aUPrMGVxu3Xr5g4ODnYnJSXZn4GGZMygP7wvIaNP2tYvf/lLd6dOnezPiHmDY26vW7eu8XH6wxmffPKJe/DgwfZvd//+/d3//Oc/mz3O3/e29+WXX9q/6Xv7f+bnpG0VFxfbvxsmCQ4NDXX37NnTfccdd7irqqp89mfEz/zj9CgdAAAAAHRErCEDAAAAAIeQkAEAAACAQ0jIAAAAAMAhJGQAAAAA4BASMgAAAABwCAkZAAAAADiEhAwAAAAAHEJCBgAAAAAOISEDAAAAAIeQkAEAcBBmz56tgIAATZ482elQAADtiJ/b7XY7HQQAAN7uiiuuUGRkpP79739r9erVSktLczokAEA7wAgZAAAHUFpaqrfffltXX321HSF75ZVXmj3+8ccfq0+fPgoNDdW4ceP06quvys/PT4WFhY3n/PDDDxo7dqzCwsLUtWtXXXfddSorK3PguwEAeBMSMgAADuCdd95R//791a9fP02ZMkUvvfSSGiaYbNy4UT//+c911llnacmSJbrqqqt0xx13NPv69evX65RTTtG5556rpUuX2uTOJGjXXnutQ98RAMBbMGURAIADOPbYY/WLX/xC119/vWpra9WpUye9++67OvHEE3Xbbbfps88+07JlyxrPv/POO/Xggw+qoKBAsbGxdrqjWX/2j3/8o/Eck5CdcMIJdpTMjKwBADomRsgAANgPs15s7ty5uuCCC+ztwMBA/fKXv7RryRoeP+KII5p9zZFHHtnsthk5M9MczRq0hsukSZPkcrnsCBsAoOMKdDoAAAC8mUm8zKhY0yIeZnJJSEiInnnmmYNeg2amMpp1Y7tLT0/3aLwAAN9CQgYAwD6YROw///mPHnvsMU2cOLHZY2bN2JtvvmnXlX3++efNHps3b16z2yNHjtTKlSvVu3fvNokbAOA7WEMGAMA+fPjhh3Z6Yk5OjmJiYpo9duutt2r69Om24IdJym688UZdfvnlWrx4sf7whz9o69attsqi+TpTyOOoo47SZZddZteTRURE2ATt66+/PuhRNgBA+8QaMgAA9jNdccKECXskY4apmDh//nyVlJTov//9r95//30NHTpUzz//fGOVRTOt0TD3z5gxQ2vWrLGl70eMGKG7776bvcwAAIyQAQDgaabC4gsvvKAtW7Y4HQoAwMuxhgwAgMP03HPP2UqLCQkJ+vHHH/Xoo4+yxxgA4KCQkAEAcJjWrl2rBx54QPn5+bZqollDdvvttzsdFgDABzBlEQAAAAAcQlEPAAAAAHAICRkAAAAAOISEDAAAAAAcQkIGAAAAAA4hIQMAAAAAh5CQAQAAAIBDSMgAAAAAwCEkZAAAAAAgZ/w/vqPZXX+am70AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Convert Spark DataFrame to Pandas for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "users_pd = updated_users.toPandas()\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=users_pd, x='Age', bins=30, kde=True)\n",
        "plt.title('Distribution of User Ages')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzYL5e3OBkAQ",
        "outputId": "dd97cd39-ba1f-43ad-f13d-963783325d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of rows: 278858\n"
          ]
        }
      ],
      "source": [
        "print(f\"number of rows: {users.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAHLlk02BkAQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdU0A6UaBkAQ"
      },
      "source": [
        "## Xử lý Rating\n",
        "Mình sẽ dùng implicit rating hệ (0,1)\n",
        "Đổi Nan, rating từ (1 ->5) là thành 0 (not good)\n",
        "Đổi 0, rating từ (5->10) là thành 1 (good)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOqCfJnIAAlu"
      },
      "source": [
        "## Xử lí user with null age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-eOLU7APxm"
      },
      "source": [
        "## xử lí tuổi bị null value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i956bGeRSqSw"
      },
      "source": [
        "# Làm việc với 3 bảng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSoj0sxntTcf",
        "outputId": "ca0c6b45-bec5-429d-abb9-e1d3fc716eb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "users_rating_df = updated_users.join(ratings, 'User-ID', 'inner')\n",
        "users_rating_df = users_rating_df.toPandas()\n",
        "books_df = books.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0rv5M6VBkAW",
        "outputId": "e098b62b-bc24-4a65-9d32-a4630b60c188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for NaN values in books_df:\n",
            "ISBN                   0\n",
            "Book-Title             0\n",
            "Book-Author            0\n",
            "Year-Of-Publication    0\n",
            "Publisher              0\n",
            "Image-URL-S            0\n",
            "Image-URL-M            0\n",
            "Image-URL-L            0\n",
            "dtype: int64\n",
            "\n",
            "No NaN values found in books_df\n"
          ]
        }
      ],
      "source": [
        "# Check for NaN values in books_df\n",
        "print(\"Checking for NaN values in books_df:\")\n",
        "print(books_df.isnull().sum())\n",
        "\n",
        "# Drop rows with any NaN values if they exist\n",
        "if books_df.isnull().any().any():\n",
        "    print(f\"\\nShape before dropping NaN values: {books_df.shape}\")\n",
        "    books_df = books_df.dropna()\n",
        "    print(f\"Shape after dropping NaN values: {books_df.shape}\")\n",
        "else:\n",
        "    print(\"\\nNo NaN values found in books_df\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ebFkJd0BkAW",
        "outputId": "7855ea3c-6fed-413b-b9e7-e7f964871f6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of users_rating_df: <class 'pandas.core.frame.DataFrame'>\n",
            "Type of books_df: <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Type of users_rating_df: {type(users_rating_df)}\")\n",
        "print(f\"Type of books_df: {type(books_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc5788O_TPbX"
      },
      "source": [
        "##Book table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "collapsed": true,
        "id": "REOsOd_sCr7d",
        "outputId": "acb2fcac-b4cb-4098-eb72-2fb50b0bde80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         ISBN                                         Book-Title  \\\n",
            "0  0195153448                                Classical Mythology   \n",
            "1  0002005018                                       Clara Callan   \n",
            "2  0060973129                               Decision in Normandy   \n",
            "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
            "4  0393045218                             The Mummies of Urumchi   \n",
            "\n",
            "            Book-Author  Year-Of-Publication                   Publisher  \\\n",
            "0    Mark P. O. Morford                 2002     Oxford University Press   \n",
            "1  Richard Bruce Wright                 2001       HarperFlamingo Canada   \n",
            "2          Carlo D'Este                 1991             HarperPerennial   \n",
            "3      Gina Bari Kolata                 1999        Farrar Straus Giroux   \n",
            "4       E. J. W. Barber                 1999  W. W. Norton &amp; Company   \n",
            "\n",
            "                                         Image-URL-S  \\\n",
            "0  http://images.amazon.com/images/P/0195153448.0...   \n",
            "1  http://images.amazon.com/images/P/0002005018.0...   \n",
            "2  http://images.amazon.com/images/P/0060973129.0...   \n",
            "3  http://images.amazon.com/images/P/0374157065.0...   \n",
            "4  http://images.amazon.com/images/P/0393045218.0...   \n",
            "\n",
            "                                         Image-URL-M  \\\n",
            "0  http://images.amazon.com/images/P/0195153448.0...   \n",
            "1  http://images.amazon.com/images/P/0002005018.0...   \n",
            "2  http://images.amazon.com/images/P/0060973129.0...   \n",
            "3  http://images.amazon.com/images/P/0374157065.0...   \n",
            "4  http://images.amazon.com/images/P/0393045218.0...   \n",
            "\n",
            "                                         Image-URL-L  \n",
            "0  http://images.amazon.com/images/P/0195153448.0...  \n",
            "1  http://images.amazon.com/images/P/0002005018.0...  \n",
            "2  http://images.amazon.com/images/P/0060973129.0...  \n",
            "3  http://images.amazon.com/images/P/0374157065.0...  \n",
            "4  http://images.amazon.com/images/P/0393045218.0...  \n"
          ]
        }
      ],
      "source": [
        "print(books_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TVAk2nYTVR2"
      },
      "source": [
        "##Users_rating table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "bY4WkKnz3IzO",
        "outputId": "387d0720-9cfd-4ec8-eff9-0c041c08044d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0060914068</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0156047624</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0245542957</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0380715899</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0553280333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   User-ID  Age        ISBN  Book-Rating\n",
              "0       53   49  0060914068            3\n",
              "1       53   49  0156047624           10\n",
              "2       53   49  0245542957            6\n",
              "3       53   49  0380715899            9\n",
              "4       53   49  0553280333            0"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "users_rating_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8mry1V8BkAX",
        "outputId": "792e1069-8eef-4245-f6f7-b7be63a50748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of rows: User-ID        1133124\n",
            "Age            1133124\n",
            "ISBN           1133124\n",
            "Book-Rating    1133124\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(f\"number of rows: {users_rating_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1YLA9AoBkAX",
        "outputId": "361723b0-55e3-4081-84bb-44b440559847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Shape before dropping unpopular books: (1022542, 11)\n",
            "Shape after dropping unpopular books: (515199, 11)\n",
            "\n",
            "NaN values in total_data_df:\n",
            "User-ID                0\n",
            "Age                    0\n",
            "ISBN                   0\n",
            "Book-Rating            0\n",
            "Book-Title             0\n",
            "Book-Author            0\n",
            "Year-Of-Publication    0\n",
            "Publisher              0\n",
            "Image-URL-S            0\n",
            "Image-URL-M            0\n",
            "Image-URL-L            0\n",
            "dtype: int64\n",
            "\n",
            "Rows with NaN values:\n",
            "Empty DataFrame\n",
            "Columns: [User-ID, Age, ISBN, Book-Rating, Book-Title, Book-Author, Year-Of-Publication, Publisher, Image-URL-S, Image-URL-M, Image-URL-L]\n",
            "Index: []\n",
            "\n",
            "Shape before dropping NaN values: (515199, 11)\n",
            "Shape after filtering: (515199, 11)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Rating</th>\n",
              "      <th>Book-Title</th>\n",
              "      <th>Book-Author</th>\n",
              "      <th>Year-Of-Publication</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Image-URL-S</th>\n",
              "      <th>Image-URL-M</th>\n",
              "      <th>Image-URL-L</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0060914068</td>\n",
              "      <td>3</td>\n",
              "      <td>Love, Medicine and Miracles</td>\n",
              "      <td>M.D. Bernie S. Siegel</td>\n",
              "      <td>1988</td>\n",
              "      <td>HarperCollins Publishers</td>\n",
              "      <td>http://images.amazon.com/images/P/0060914068.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060914068.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060914068.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>49</td>\n",
              "      <td>0380715899</td>\n",
              "      <td>9</td>\n",
              "      <td>A Soldier of the Great War</td>\n",
              "      <td>Mark Helprin</td>\n",
              "      <td>1992</td>\n",
              "      <td>Avon Books</td>\n",
              "      <td>http://images.amazon.com/images/P/0380715899.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0380715899.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0380715899.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>85</td>\n",
              "      <td>41</td>\n",
              "      <td>0340767936</td>\n",
              "      <td>5</td>\n",
              "      <td>Turning Thirty</td>\n",
              "      <td>Mike Gayle</td>\n",
              "      <td>2000</td>\n",
              "      <td>Hodder &amp;amp; Stoughton General Division</td>\n",
              "      <td>http://images.amazon.com/images/P/0340767936.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0340767936.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0340767936.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>243</td>\n",
              "      <td>41</td>\n",
              "      <td>0060915544</td>\n",
              "      <td>10</td>\n",
              "      <td>The Bean Trees</td>\n",
              "      <td>Barbara Kingsolver</td>\n",
              "      <td>1989</td>\n",
              "      <td>Perennial</td>\n",
              "      <td>http://images.amazon.com/images/P/0060915544.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060915544.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060915544.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>243</td>\n",
              "      <td>41</td>\n",
              "      <td>0060977493</td>\n",
              "      <td>7</td>\n",
              "      <td>The God of Small Things</td>\n",
              "      <td>Arundhati Roy</td>\n",
              "      <td>1998</td>\n",
              "      <td>Perennial</td>\n",
              "      <td>http://images.amazon.com/images/P/0060977493.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060977493.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0060977493.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    User-ID  Age        ISBN  Book-Rating                   Book-Title  \\\n",
              "0        53   49  0060914068            3  Love, Medicine and Miracles   \n",
              "3        53   49  0380715899            9   A Soldier of the Great War   \n",
              "7        85   41  0340767936            5               Turning Thirty   \n",
              "9       243   41  0060915544           10               The Bean Trees   \n",
              "10      243   41  0060977493            7      The God of Small Things   \n",
              "\n",
              "              Book-Author  Year-Of-Publication  \\\n",
              "0   M.D. Bernie S. Siegel                 1988   \n",
              "3            Mark Helprin                 1992   \n",
              "7              Mike Gayle                 2000   \n",
              "9      Barbara Kingsolver                 1989   \n",
              "10          Arundhati Roy                 1998   \n",
              "\n",
              "                                  Publisher  \\\n",
              "0                  HarperCollins Publishers   \n",
              "3                                Avon Books   \n",
              "7   Hodder &amp; Stoughton General Division   \n",
              "9                                 Perennial   \n",
              "10                                Perennial   \n",
              "\n",
              "                                          Image-URL-S  \\\n",
              "0   http://images.amazon.com/images/P/0060914068.0...   \n",
              "3   http://images.amazon.com/images/P/0380715899.0...   \n",
              "7   http://images.amazon.com/images/P/0340767936.0...   \n",
              "9   http://images.amazon.com/images/P/0060915544.0...   \n",
              "10  http://images.amazon.com/images/P/0060977493.0...   \n",
              "\n",
              "                                          Image-URL-M  \\\n",
              "0   http://images.amazon.com/images/P/0060914068.0...   \n",
              "3   http://images.amazon.com/images/P/0380715899.0...   \n",
              "7   http://images.amazon.com/images/P/0340767936.0...   \n",
              "9   http://images.amazon.com/images/P/0060915544.0...   \n",
              "10  http://images.amazon.com/images/P/0060977493.0...   \n",
              "\n",
              "                                          Image-URL-L  \n",
              "0   http://images.amazon.com/images/P/0060914068.0...  \n",
              "3   http://images.amazon.com/images/P/0380715899.0...  \n",
              "7   http://images.amazon.com/images/P/0340767936.0...  \n",
              "9   http://images.amazon.com/images/P/0060915544.0...  \n",
              "10  http://images.amazon.com/images/P/0060977493.0...  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create total data dataframe\n",
        "total_data_df = pd.merge(users_rating_df, books_df, on='ISBN', how='inner', validate='m:m')\n",
        "\n",
        "# Count ratings per book\n",
        "book_ratings_count = total_data_df.groupby('ISBN')['Book-Rating'].count()\n",
        "\n",
        "# Get books with less than 50 ratings\n",
        "books_to_drop = book_ratings_count[book_ratings_count < 10].index\n",
        "\n",
        "# Drop books with less than 50 ratings\n",
        "print(f\"\\nShape before dropping unpopular books: {total_data_df.shape}\")\n",
        "total_data_df = total_data_df[~total_data_df['ISBN'].isin(books_to_drop)]\n",
        "print(f\"Shape after dropping unpopular books: {total_data_df.shape}\")\n",
        "\n",
        "# Check for NaN values in total_data_df\n",
        "print(\"\\nNaN values in total_data_df:\")\n",
        "print(total_data_df.isnull().sum())\n",
        "# Show rows with NaN values\n",
        "print(\"\\nRows with NaN values:\")\n",
        "print(total_data_df[total_data_df.isnull().any(axis=1)].head())\n",
        "# Drop NaN values from total_data_df\n",
        "filtered_data_df = total_data_df.dropna()\n",
        "print(\"\\nShape before dropping NaN values:\", total_data_df.shape)\n",
        "\n",
        "print(f\"Shape after filtering: {filtered_data_df.shape}\")\n",
        "filtered_data_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_rAM6WVc-p"
      },
      "source": [
        "# Colaborative filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnK122SGVm16"
      },
      "source": [
        "## Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJZKWJZUfkb6",
        "outputId": "02b2e5d1-970a-4352-e62d-7fdb383adb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- User-ID: long (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Rating: long (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: long (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "ratings_spark_df = spark.createDataFrame(filtered_data_df)\n",
        "ratings_spark_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IughzAEiTZrR",
        "outputId": "92aa02e0-f2df-4795-a3f1-ea11b90fd04d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "isbn_indexer = StringIndexer(inputCol=\"ISBN\", outputCol=\"bookId\")\n",
        "ratings_indexed = isbn_indexer.fit(ratings_spark_df).transform(ratings_spark_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oysz_F7bTbkU",
        "outputId": "3bd7bc5c-4b9f-4263-d971-3bb8bff87839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training data size: 412362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 104518:=========================================>       (170 + 10) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample training data size for CV: 82976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "(training, test) = ratings_indexed.randomSplit([0.8, 0.2], seed=42)\n",
        "# Take only 20% of training data for cross validation to speed up the process\n",
        "training_sample = training.sample(fraction=0.2, seed=42)\n",
        "print(f\"Original training data size: {training.count()}\")\n",
        "print(f\"Sample training data size for CV: {training_sample.count()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfgN8nUPBkAY"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# 1. Create the ALS model\n",
        "als = ALS(\n",
        "    maxIter=10,\n",
        "    regParam=0.01,\n",
        "    rank=10,\n",
        "    alpha= 40,\n",
        "    userCol=\"User-ID\",\n",
        "    itemCol=\"bookId\",\n",
        "    ratingCol=\"Book-Rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True,  # Ensures non-negative predictions\n",
        "    implicitPrefs=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH9-udcnBkAY"
      },
      "outputs": [],
      "source": [
        "# 2. Define parameter grid for tuning\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [15, 20, 25]) \\\n",
        "    .addGrid(als.regParam, [0.05, 0.1, 0.15]) \\\n",
        "    .addGrid(als.maxIter, [15, 20, 25]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw1ckY1lBkAY"
      },
      "outputs": [],
      "source": [
        "# 3. Define evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"Book-Rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8z_-4t6BkAY"
      },
      "outputs": [],
      "source": [
        "# 4. Create CrossValidator\n",
        "cv = CrossValidator(\n",
        "    estimator=als,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ArzOijgBkAY",
        "outputId": "0ca4b8f2-490a-4e7c-fae0-e372aaf41801"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 12:07:41 WARN CacheManager: Asked to cache already cached data.\n",
            "25/05/27 12:07:41 WARN CacheManager: Asked to cache already cached data.\n",
            "25/05/27 12:07:41 WARN CacheManager: Asked to cache already cached data.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with cross-validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 6. Cache the datasets\n",
        "training.cache()\n",
        "test.cache()\n",
        "training_sample.cache()\n",
        "\n",
        "# 7. Fit the model using cross-validation\n",
        "print(\"Training model with cross-validation...\")\n",
        "cvModel = cv.fit(training_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uidof07uBkAY",
        "outputId": "f0017999-0f21-4054-f250-d29db33175c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best Model Parameters:\n",
            "Rank: 15\n",
            "\n",
            "Best parameter combination (index 0):\n",
            "rank: 15\n",
            "regParam: 0.05\n",
            "maxIter: 25\n",
            "\n",
            "Best cross-validation score: 4.575627743042291\n"
          ]
        }
      ],
      "source": [
        "# 8. Get the best model\n",
        "bestModel = cvModel.bestModel\n",
        "\n",
        "# 9. Print the best parameters\n",
        "print(\"\\nBest Model Parameters:\")\n",
        "print(f\"Rank: {bestModel.rank}\")\n",
        "\n",
        "# Get all parameter maps and find the best one\n",
        "param_maps = cvModel.getEstimatorParamMaps()\n",
        "avg_metrics = cvModel.avgMetrics\n",
        "\n",
        "# Find the index of the best model (lowest error)\n",
        "best_index = avg_metrics.index(min(avg_metrics))\n",
        "best_param_map = param_maps[best_index]\n",
        "\n",
        "print(f\"\\nBest parameter combination (index {best_index}):\")\n",
        "for param, value in best_param_map.items():\n",
        "    print(f\"{param.name}: {value}\")\n",
        "\n",
        "print(f\"\\nBest cross-validation score: {min(avg_metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8p9ZqqxBkAY"
      },
      "outputs": [],
      "source": [
        "# 10. Make predictions on validation set\n",
        "predictions = bestModel.transform(training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV0rHtsyBkAZ",
        "outputId": "3b596742-958c-4a0c-efe3-245cfeea65b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Root-mean-square error on validation set = 4.5801805519596135\n",
            "\n",
            "Sample Predictions:\n",
            "+-------+----------+-----------+----------+\n",
            "|User-ID|      ISBN|Book-Rating|prediction|\n",
            "+-------+----------+-----------+----------+\n",
            "|   1591|3257228007|         10| 0.6514497|\n",
            "|   4900|0312282990|          7|       0.0|\n",
            "|   4900|0316776963|          7|       0.0|\n",
            "|   4900|0375704027|          0|       0.0|\n",
            "|   4900|078686351X|          0|       0.0|\n",
            "+-------+----------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 11. Evaluate final model\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"\\nRoot-mean-square error on validation set = {rmse}\")\n",
        "\n",
        "# 12. Show sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "predictions.select(\"User-ID\", \"ISBN\", \"Book-Rating\", \"prediction\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBLCl5dDBkAZ",
        "outputId": "ef913569-21ce-4d2b-fd4e-72861d5f02f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows in predictions: 348573\n",
            "Sample of predictions:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+-----------+--------------------+---------------+-------------------+-----------------+--------------------+--------------------+--------------------+------+----------+\n",
            "|User-ID|Age|      ISBN|Book-Rating|          Book-Title|    Book-Author|Year-Of-Publication|        Publisher|         Image-URL-S|         Image-URL-M|         Image-URL-L|bookId|prediction|\n",
            "+-------+---+----------+-----------+--------------------+---------------+-------------------+-----------------+--------------------+--------------------+--------------------+------+----------+\n",
            "|   1591| 24|3257228007|         10|Das Parfum: Die G...|Patrick Suskind|               1994|  Distribooks Inc|http://images.ama...|http://images.ama...|http://images.ama...|1972.0| 0.6514497|\n",
            "|   4900| 26|0312282990|          7|The Amazing Adven...| Michael Chabon|               2001|          Picador|http://images.ama...|http://images.ama...|http://images.ama...| 194.0|       0.0|\n",
            "|   4900| 26|0316776963|          7|Me Talk Pretty On...|  David Sedaris|               2001|   Back Bay Books|http://images.ama...|http://images.ama...|http://images.ama...|  64.0|       0.0|\n",
            "|   4900| 26|0375704027|          0|Norwegian Wood (V...|Haruki Murakami|               2000|Vintage Books USA|http://images.ama...|http://images.ama...|http://images.ama...|3019.0|       0.0|\n",
            "|   4900| 26|078686351X|          0|Dirty Jokes and B...|     Drew Carey|               1997|   Hyperion Books|http://images.ama...|http://images.ama...|http://images.ama...|5373.0|       0.0|\n",
            "+-------+---+----------+-----------+--------------------+---------------+-------------------+-----------------+--------------------+--------------------+--------------------+------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of null values in prediction column: 0\n",
            "Number of null values in Age column: 0\n",
            "Schema of predictions DataFrame:\n",
            "root\n",
            " |-- User-ID: long (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- ISBN: string (nullable = true)\n",
            " |-- Book-Rating: long (nullable = true)\n",
            " |-- Book-Title: string (nullable = true)\n",
            " |-- Book-Author: string (nullable = true)\n",
            " |-- Year-Of-Publication: long (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- Image-URL-S: string (nullable = true)\n",
            " |-- Image-URL-M: string (nullable = true)\n",
            " |-- Image-URL-L: string (nullable = true)\n",
            " |-- bookId: double (nullable = false)\n",
            " |-- prediction: float (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add these debug prints after your predictions\n",
        "print(\"Number of rows in predictions:\", predictions.count())\n",
        "print(\"Sample of predictions:\")\n",
        "predictions.show(5)\n",
        "\n",
        "# Check for any null values\n",
        "print(\"Number of null values in prediction column:\", predictions.filter(col(\"prediction\").isNull()).count())\n",
        "print(\"Number of null values in Age column:\", predictions.filter(col(\"Age\").isNull()).count())\n",
        "\n",
        "# Check the data types\n",
        "print(\"Schema of predictions DataFrame:\")\n",
        "predictions.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Pe_-GvV3I0",
        "outputId": "047d49ef-8f51-418e-d272-676e44fa280d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 117656:==========================================>       (84 + 10) / 100]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|User-ID|recommendations                                                                                                                                                                                     |\n",
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|243    |[{5, 1.4405113}, {22, 1.3647817}, {55, 1.2382241}, {40, 1.2334836}, {12, 1.1807181}, {198, 1.1145091}, {197, 1.08782}, {11, 1.0672448}, {101, 1.0482694}, {446, 1.0298691}]                         |\n",
            "|513    |[{33, 0.96017236}, {51, 0.89654464}, {84, 0.86698484}, {68, 0.8434433}, {171, 0.84169495}, {237, 0.7902049}, {194, 0.76212573}, {251, 0.7525683}, {1334, 0.7110178}, {443, 0.70192933}]             |\n",
            "|883    |[{0, 1.0862166}, {11, 0.85297334}, {25, 0.6989272}, {90, 0.69631696}, {416, 0.6451855}, {248, 0.6440626}, {156, 0.623842}, {1172, 0.60119104}, {413, 0.5854343}, {854, 0.584925}]                   |\n",
            "|1025   |[{10, 1.3200785}, {68, 0.9801842}, {62, 0.9771493}, {33, 0.9754374}, {26, 0.9044685}, {84, 0.9043045}, {100, 0.8866306}, {21, 0.872535}, {51, 0.8693824}, {3, 0.867629}]                            |\n",
            "|1591   |[{0, 1.3000934}, {11, 1.0209244}, {25, 0.83654654}, {90, 0.8334223}, {416, 0.772223}, {248, 0.77087903}, {156, 0.746677}, {1172, 0.71956605}, {413, 0.7007068}, {854, 0.7000972}]                   |\n",
            "|1721   |[{0, 1.1429487}, {11, 0.89752346}, {25, 0.7354316}, {90, 0.732685}, {416, 0.6843491}, {248, 0.6779215}, {156, 0.6564248}, {66, 0.64359015}, {167, 0.6354247}, {1172, 0.63471496}]                   |\n",
            "|1903   |[{33, 0.9264293}, {51, 0.86503756}, {84, 0.8365166}, {68, 0.81380236}, {171, 0.8121155}, {237, 0.7624349}, {194, 0.7353425}, {251, 0.72612095}, {1334, 0.6860306}, {443, 0.6772616}]                |\n",
            "|2559   |[{5, 0.7315272}, {22, 0.645806}, {55, 0.56131613}, {12, 0.53664535}, {40, 0.5319686}, {198, 0.48373556}, {197, 0.47250968}, {446, 0.45744064}, {101, 0.4479299}, {274, 0.44119626}]                 |\n",
            "|2776   |[{23, 0.8929261}, {28, 0.87622625}, {69, 0.8394127}, {57, 0.8217958}, {60, 0.81741184}, {83, 0.7746451}, {88, 0.75155747}, {184, 0.74267596}, {140, 0.72150236}, {108, 0.7141345}]                  |\n",
            "|2996   |[{78, 0.58500046}, {9, 0.5549227}, {228, 0.48923868}, {159, 0.47530314}, {143, 0.46180856}, {0, 0.43616524}, {361, 0.43548977}, {41, 0.43079245}, {60, 0.42582718}, {710, 0.42576936}]              |\n",
            "|3761   |[{165, 0.79301804}, {309, 0.78039366}, {123, 0.77202666}, {209, 0.7371076}, {205, 0.7105243}, {178, 0.6921816}, {699, 0.6812269}, {300, 0.6759555}, {549, 0.6676613}, {132, 0.6574489}]             |\n",
            "|4092   |[{9, 0.76487184}, {159, 0.5725521}, {78, 0.55012345}, {143, 0.527599}, {710, 0.48457593}, {217, 0.47381523}, {119, 0.46746612}, {541, 0.4628295}, {260, 0.4580139}, {602, 0.4553381}]               |\n",
            "|4489   |[{99, 0.0}, {98, 0.0}, {97, 0.0}, {96, 0.0}, {95, 0.0}, {94, 0.0}, {93, 0.0}, {92, 0.0}, {91, 0.0}, {90, 0.0}]                                                                                      |\n",
            "|4900   |[{99, 0.0}, {98, 0.0}, {97, 0.0}, {96, 0.0}, {95, 0.0}, {94, 0.0}, {93, 0.0}, {92, 0.0}, {91, 0.0}, {90, 0.0}]                                                                                      |\n",
            "|4929   |[{99, 0.0}, {98, 0.0}, {97, 0.0}, {96, 0.0}, {95, 0.0}, {94, 0.0}, {93, 0.0}, {92, 0.0}, {91, 0.0}, {90, 0.0}]                                                                                      |\n",
            "|5030   |[{35, 9.161845E-8}, {47, 8.855727E-8}, {70, 8.767042E-8}, {42, 8.681199E-8}, {14, 7.611127E-8}, {96, 7.171564E-8}, {307, 6.8056586E-8}, {120, 6.606956E-8}, {588, 6.5446585E-8}, {164, 6.494957E-8}]|\n",
            "|5100   |[{20, 0.8817086}, {52, 0.7941798}, {49, 0.7162855}, {17, 0.7126359}, {126, 0.7037724}, {44, 0.67121166}, {256, 0.649367}, {117, 0.63284844}, {357, 0.62754273}, {304, 0.6036515}]                   |\n",
            "|5287   |[{33, 1.0154525}, {51, 0.9481614}, {84, 0.9168998}, {68, 0.8920029}, {171, 0.89015394}, {237, 0.83569944}, {194, 0.80600363}, {251, 0.79589593}, {1334, 0.75195324}, {443, 0.7423415}]              |\n",
            "|5543   |[{20, 1.7274683}, {52, 1.5770136}, {49, 1.4223379}, {17, 1.4040474}, {126, 1.3974905}, {44, 1.2956996}, {256, 1.2817672}, {117, 1.2566557}, {357, 1.2461201}, {136, 1.193513}]                      |\n",
            "|5920   |[{99, 0.0}, {98, 0.0}, {97, 0.0}, {96, 0.0}, {95, 0.0}, {94, 0.0}, {93, 0.0}, {92, 0.0}, {91, 0.0}, {90, 0.0}]                                                                                      |\n",
            "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Recommend top 10 books for each user\n",
        "user_recs = bestModel.recommendForAllUsers(10)\n",
        "user_recs.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIwa--uTeURL",
        "outputId": "6d10cebd-7748-4eb7-b8d1-74ed18bf233f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                ]22]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------------+---------------------------------+-------------+----------------+\n",
            "|User-ID|Recommended_ISBN|Book-Title                       |Book-Author  |Estimated_Rating|\n",
            "+-------+----------------+---------------------------------+-------------+----------------+\n",
            "|6142   |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.90238404      |\n",
            "|17836  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.8716409       |\n",
            "|18979  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.7332891       |\n",
            "|24926  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.80282223      |\n",
            "|25203  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|3.236014E-8     |\n",
            "|26120  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.8219905       |\n",
            "|32855  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|6.020272E-12    |\n",
            "|36130  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.32047328      |\n",
            "|36523  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.39268252      |\n",
            "|40893  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.6809194       |\n",
            "|59180  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.7087264       |\n",
            "|64535  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|1.0489428       |\n",
            "|66978  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.5763436       |\n",
            "|67376  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|1.0669664       |\n",
            "|69341  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.6090598       |\n",
            "|75149  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|1.2695694       |\n",
            "|82901  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.8459828       |\n",
            "|87301  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.7930522       |\n",
            "|94723  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.70402014      |\n",
            "|96554  |0060976845      |Little Altars Everywhere: A Novel|Rebecca Wells|0.88202953      |\n",
            "+-------+----------------+---------------------------------+-------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have `books` and `ratings_indexed` Spark DataFrames from the preceding code\n",
        "# You also need the `isbn_indexer` fitted from the previous step to get the original ISBNs\n",
        "\n",
        "# Get the mapping from indexed bookId back to original ISBN\n",
        "# This can be done by accessing the labels from the fitted StringIndexer model\n",
        "# You need to refit the indexer to get the labels again if the previous fit was not saved\n",
        "fitted_isbn_indexer = isbn_indexer.fit(ratings_spark_df)\n",
        "book_id_to_isbn = fitted_isbn_indexer.labels\n",
        "\n",
        "# Create a Spark DataFrame for the book ID to ISBN mapping\n",
        "# The index of the label corresponds to the indexed bookId\n",
        "# Ensure the data type matches the bookId in the recommendations struct (IntegerType)\n",
        "from pyspark.sql.types import IntegerType\n",
        "book_id_mapping = spark.createDataFrame([(i, isbn) for i, isbn in enumerate(book_id_to_isbn)], [\"bookId\", \"Original_ISBN\"])\n",
        "book_id_mapping = book_id_mapping.withColumn(\"bookId\", col(\"bookId\").cast(IntegerType())) # Cast to IntegerType\n",
        "\n",
        "# Now, flatten the recommendations list to join with the main books DataFrame\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "user_recs_exploded = user_recs.select(\"User-ID\", explode(\"recommendations\").alias(\"recommendation\"))\n",
        "\n",
        "# Join with the book ID to ISBN mapping\n",
        "# Access the bookId from the exploded 'recommendation' struct\n",
        "user_recs_with_isbn = user_recs_exploded.join(book_id_mapping, user_recs_exploded.recommendation.getItem(\"bookId\") == book_id_mapping.bookId, \"inner\")\n",
        "\n",
        "# Join with the original books DataFrame to get book details\n",
        "# Ensure the 'ISBN' column in the 'books' DataFrame is consistent\n",
        "# Assuming 'ISBN' is the correct column name in your original books Spark DataFrame\n",
        "book_details_recs = user_recs_with_isbn.join(books, user_recs_with_isbn.Original_ISBN == books.ISBN, \"inner\")\n",
        "\n",
        "# Select relevant columns\n",
        "final_recommendations = book_details_recs.select(\n",
        "    user_recs_with_isbn[\"User-ID\"],\n",
        "    user_recs_with_isbn.Original_ISBN.alias(\"Recommended_ISBN\"),\n",
        "    books[\"Book-Title\"],\n",
        "    books[\"Book-Author\"],\n",
        "    user_recs_with_isbn.recommendation.getItem(\"rating\").alias(\"Estimated_Rating\")\n",
        ")\n",
        "\n",
        "# Show the final recommendations with book details\n",
        "final_recommendations.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kDBZYqvBkAZ",
        "outputId": "39a2d678-2899-4e67-9ae4-72f6169f6f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating Precision@K, Recall@K, and NDCG@K...\n",
            "\n",
            "--- Rating Threshold: 0.3 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@10: 0.0026\n",
            "Recall@10: 0.0087\n",
            "NDCG@10: 0.0062\n",
            "F1-Score@10: 0.0040\n",
            "\n",
            "--- Rating Threshold: 0.5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@10: 0.0026\n",
            "Recall@10: 0.0087\n",
            "NDCG@10: 0.0062\n",
            "F1-Score@10: 0.0040\n",
            "\n",
            "--- Rating Threshold: 0.7 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 128934:=========================================>        (83 + 10) / 100]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@10: 0.0026\n",
            "Recall@10: 0.0087\n",
            "NDCG@10: 0.0062\n",
            "F1-Score@10: 0.0040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Calculate Precision@K, Recall@K, NDCG@K of the model\n",
        "from pyspark.sql.functions import collect_list, size, array_intersect, col\n",
        "import math\n",
        "\n",
        "def calculate_precision_recall_ndcg(model, test_data, k=15, rating_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate Precision@K, Recall@K, and NDCG@K for the recommendation model\n",
        "\n",
        "    Args:\n",
        "        model: Trained ALS model\n",
        "        test_data: Test dataset with columns [User-ID, bookId, Book-Rating]\n",
        "        k: Number of recommendations to evaluate\n",
        "        rating_threshold: Minimum rating to consider as \"relevant/positive\"\n",
        "    \"\"\"\n",
        "\n",
        "    # FIXED: Only consider books with ratings above threshold as relevant\n",
        "    user_actual_books = test_data.filter(col(\"Book-Rating\") >= rating_threshold) \\\n",
        "        .groupBy(\"User-ID\") \\\n",
        "        .agg(collect_list(\"bookId\").alias(\"actual_books\"))\n",
        "\n",
        "    # Get unique users from test data (users who have at least one positive rating)\n",
        "    users = user_actual_books.select(\"User-ID\").distinct()\n",
        "\n",
        "    # Generate recommendations for all users\n",
        "    user_recommendations = model.recommendForUserSubset(users, k)\n",
        "\n",
        "    # Extract recommended book IDs from recommendations\n",
        "    from pyspark.sql.functions import expr\n",
        "    user_recommendations = user_recommendations.withColumn(\n",
        "        \"recommended_books\",\n",
        "        expr(\"transform(recommendations, x -> x.bookId)\")\n",
        "    )\n",
        "\n",
        "    # Join recommendations with actual ratings\n",
        "    evaluation_data = user_recommendations.join(user_actual_books, \"User-ID\")\n",
        "\n",
        "    # Calculate metrics for each user\n",
        "    def calculate_metrics_udf(recommended, actual):\n",
        "        if not recommended or not actual:\n",
        "            return (0.0, 0.0, 0.0)\n",
        "\n",
        "        recommended_set = set(recommended)\n",
        "        actual_set = set(actual)\n",
        "\n",
        "        # Calculate intersection (true positives)\n",
        "        intersection = recommended_set.intersection(actual_set)\n",
        "\n",
        "        # Precision@K: What fraction of recommended items are relevant?\n",
        "        precision = len(intersection) / len(recommended) if len(recommended) > 0 else 0.0\n",
        "\n",
        "        # Recall@K: What fraction of relevant items are recommended?\n",
        "        recall = len(intersection) / len(actual) if len(actual) > 0 else 0.0\n",
        "\n",
        "        # NDCG@K calculation\n",
        "        # Create relevance scores (1 for relevant, 0 for not relevant)\n",
        "        relevance_scores = [1.0 if book_id in actual_set else 0.0 for book_id in recommended]\n",
        "\n",
        "        # Calculate DCG@K\n",
        "        dcg = sum(rel / math.log(i + 2, 2) for i, rel in enumerate(relevance_scores))\n",
        "\n",
        "        # Calculate IDCG@K (ideal DCG)\n",
        "        ideal_relevance = [1.0] * min(len(actual), k)\n",
        "        idcg = sum(rel / math.log(i + 2, 2) for i, rel in enumerate(ideal_relevance))\n",
        "\n",
        "        # NDCG@K\n",
        "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "        return (precision, recall, ndcg)\n",
        "\n",
        "    # Register UDF\n",
        "    from pyspark.sql.functions import udf\n",
        "    from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "\n",
        "    metrics_schema = StructType([\n",
        "        StructField(\"precision\", DoubleType(), True),\n",
        "        StructField(\"recall\", DoubleType(), True),\n",
        "        StructField(\"ndcg\", DoubleType(), True)\n",
        "    ])\n",
        "\n",
        "    metrics_udf = udf(calculate_metrics_udf, metrics_schema)\n",
        "\n",
        "    # Calculate metrics for each user\n",
        "    user_metrics = evaluation_data.withColumn(\n",
        "        \"metrics\",\n",
        "        metrics_udf(col(\"recommended_books\"), col(\"actual_books\"))\n",
        "    ).select(\n",
        "        col(\"User-ID\"),\n",
        "        col(\"metrics.precision\").alias(\"precision\"),\n",
        "        col(\"metrics.recall\").alias(\"recall\"),\n",
        "        col(\"metrics.ndcg\").alias(\"ndcg\")\n",
        "    )\n",
        "\n",
        "    # Calculate average metrics\n",
        "    from pyspark.sql.functions import avg\n",
        "    avg_metrics = user_metrics.agg(\n",
        "        avg(\"precision\").alias(\"avg_precision\"),\n",
        "        avg(\"recall\").alias(\"avg_recall\"),\n",
        "        avg(\"ndcg\").alias(\"avg_ndcg\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    return {\n",
        "        'precision@k': avg_metrics['avg_precision'],\n",
        "        'recall@k': avg_metrics['avg_recall'],\n",
        "        'ndcg@k': avg_metrics['avg_ndcg']\n",
        "    }\n",
        "\n",
        "# Calculate metrics with different thresholds\n",
        "print(\"Calculating Precision@K, Recall@K, and NDCG@K...\")\n",
        "\n",
        "# Try different rating thresholds to see what makes sense for your data\n",
        "for threshold in [0.3, 0.5, 0.7]:\n",
        "    print(f\"\\n--- Rating Threshold: {threshold} ---\")\n",
        "    metrics = calculate_precision_recall_ndcg(bestModel, test, k=10, rating_threshold=threshold)\n",
        "\n",
        "    print(f\"Precision@10: {metrics['precision@k']:.4f}\")\n",
        "    print(f\"Recall@10: {metrics['recall@k']:.4f}\")\n",
        "    print(f\"NDCG@10: {metrics['ndcg@k']:.4f}\")\n",
        "\n",
        "    # Calculate F1-Score\n",
        "    f1_score = 2 * (metrics['precision@k'] * metrics['recall@k']) / (metrics['precision@k'] + metrics['recall@k']) if (metrics['precision@k'] + metrics['recall@k']) > 0 else 0\n",
        "    print(f\"F1-Score@10: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQCznhvBBkAZ",
        "outputId": "269e22bb-c42b-45dc-aea3-320ccea31700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create mapping between numeric IDs and ISBNs\n",
        "# Assuming you have the original mapping from when you created the model\n",
        "fitted_isbn_indexer = isbn_indexer.fit(ratings_spark_df)\n",
        "book_id_to_isbn = fitted_isbn_indexer.labels\n",
        "\n",
        "# Create a reverse mapping (ISBN to numeric ID)\n",
        "isbn_to_id = {isbn: idx for idx, isbn in enumerate(book_id_to_isbn)}\n",
        "\n",
        "def get_reranked_recommendations(user_id, current_book_isbn, num_recommendations=10):\n",
        "    # Convert ISBN to numeric ID\n",
        "    current_book_id = isbn_to_id.get(current_book_isbn)\n",
        "    if current_book_id is None:\n",
        "        print(f\"Book ISBN {current_book_isbn} not found in the mapping\")\n",
        "        return None\n",
        "    print(f\"Current book ID: {current_book_id}\")\n",
        "\n",
        "    # Step 1: Get initial ALS recommendations\n",
        "    try:\n",
        "        user_recs = bestModel.recommendForUserSubset(\n",
        "            spark.createDataFrame([(user_id,)], [\"User-ID\"]),\n",
        "            num_recommendations\n",
        "        )\n",
        "        user_recs = user_recs.collect()\n",
        "\n",
        "        if not user_recs:\n",
        "            print(f\"No recommendations found for user {user_id}\")\n",
        "            return None\n",
        "\n",
        "        # Get recommendations list\n",
        "        recommendations = user_recs[0].recommendations\n",
        "\n",
        "        # Step 2: Get the current book's features (OPTIMIZED)\n",
        "        # Filter itemFactors first instead of collecting all and iterating\n",
        "        item_factors_df = bestModel.itemFactors.filter(bestModel.itemFactors.id == current_book_id)\n",
        "        if item_factors_df.count() > 0:\n",
        "            current_book_features = np.array(item_factors_df.first().features)\n",
        "        else:\n",
        "            print(f\"Current book ID {current_book_id} not found in the model\")\n",
        "            return None\n",
        "\n",
        "        # Step 3: Calculate similarities and create a list of (ISBN, rating, similarity)\n",
        "        # Get all recommended book IDs for batch filtering\n",
        "        recommended_book_ids = [rec.bookId for rec in recommendations]\n",
        "\n",
        "        # Filter itemFactors for all recommended books at once (OPTIMIZED)\n",
        "        recommended_factors_df = bestModel.itemFactors.filter(\n",
        "            bestModel.itemFactors.id.isin(recommended_book_ids)\n",
        "        ).collect()\n",
        "\n",
        "        # Create a lookup dictionary for faster access\n",
        "        book_features_lookup = {item.id: np.array(item.features) for item in recommended_factors_df}\n",
        "\n",
        "        recommendations_with_similarity = []\n",
        "        for rec in recommendations:\n",
        "            rec_features = book_features_lookup.get(rec.bookId)\n",
        "\n",
        "            if rec_features is not None:\n",
        "                # Calculate cosine similarity using numpy\n",
        "                dot_product = np.dot(current_book_features, rec_features)\n",
        "                norm_current = np.linalg.norm(current_book_features)\n",
        "                norm_rec = np.linalg.norm(rec_features)\n",
        "                similarity = float(dot_product / (norm_current * norm_rec))\n",
        "\n",
        "                # Convert numeric ID back to ISBN for display\n",
        "                isbn = book_id_to_isbn[rec.bookId]\n",
        "                recommendations_with_similarity.append((isbn, rec.rating, similarity))\n",
        "            else:\n",
        "                print(f\"Could not find features for bookId: {rec.bookId}\")\n",
        "\n",
        "        if not recommendations_with_similarity:\n",
        "            print(\"No similar books found\")\n",
        "            return None\n",
        "\n",
        "        # Step 4: Create DataFrames for recommendations and similarities\n",
        "        recommendations_df = spark.createDataFrame(\n",
        "            [(isbn, rating) for isbn, rating, _ in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"rating\"]\n",
        "        )\n",
        "\n",
        "        similarities_df = spark.createDataFrame(\n",
        "            [(isbn, similarity) for isbn, _, similarity in recommendations_with_similarity],\n",
        "            [\"ISBN\", \"Similarity\"]\n",
        "        )\n",
        "\n",
        "        # Step 5: Join with book details and similarities\n",
        "        book_details_recs = recommendations_df.join(\n",
        "            books,\n",
        "            recommendations_df.ISBN == books.ISBN,\n",
        "            \"inner\"\n",
        "        ).join(\n",
        "            similarities_df,\n",
        "            recommendations_df.ISBN == similarities_df.ISBN,\n",
        "            \"inner\"\n",
        "        )\n",
        "\n",
        "        # Step 6: Calculate final score (combine ALS rating and similarity)\n",
        "        als_weight = 0.6\n",
        "        similarity_weight = 0.4\n",
        "\n",
        "        final_recommendations = book_details_recs.select(\n",
        "            books[\"ISBN\"],\n",
        "            books[\"Book-Title\"],\n",
        "            books[\"Book-Author\"],\n",
        "            recommendations_df[\"rating\"].alias(\"ALS_Rating\"),\n",
        "            similarities_df[\"Similarity\"],\n",
        "            (recommendations_df[\"rating\"] * als_weight +\n",
        "             similarities_df[\"Similarity\"] * similarity_weight).alias(\"Final_Score\")\n",
        "        ).orderBy(\"Final_Score\", ascending=False)\n",
        "        # Filter recommendations to only include those with similarity > 0.5\n",
        "        final_recommendations = final_recommendations.filter(\n",
        "            similarities_df[\"Similarity\"] > 0.3\n",
        "        )\n",
        "        return final_recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting recommendations: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVOS8JqFBkAZ",
        "outputId": "fad96def-cd44-4ee9-b72b-f9ed4d91e158"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating recommendations for 50 users and 50 books...\n",
            "Current book ID: 12539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 129594:=========================================>       (171 + 10) / 200]/ 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 130134:================================================> (195 + 5) / 200] 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 130674:=================================================>(196 + 4) / 200] 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 131161:=============================================>   (186 + 11) / 200] 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 6463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 131648:===========================================>     (177 + 10) / 200]/ 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 12539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 132188:=========================================>       (169 + 13) / 200]/ 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 132728:=============================================>   (187 + 10) / 200] 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 133268:===============================================>  (191 + 9) / 200] 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 6463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 12539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 14450\n",
            "Current book ID: 387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 14450\n",
            "Current book ID: 408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 14450\n",
            "Current book ID: 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 14450\n",
            "Current book ID: 6463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 14450\n",
            "Current book ID: 12539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 6463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 200]]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current book ID: 12539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 12799\n",
            "Current book ID: 387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 12799\n",
            "Current book ID: 408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 12799\n",
            "Current book ID: 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 12799\n",
            "Current book ID: 6463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 137981:======================================>          (159 + 10) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No recommendations found for user 12799\n",
            "\n",
            "Finished generating recommendations\n",
            "\n",
            "Sample of recommendations:\n",
            "         ISBN                                         Book-Title  \\\n",
            "0  0345353145                                             Sphere   \n",
            "1  0345353145                                             Sphere   \n",
            "2  0060928336    Divine Secrets of the Ya-Ya Sisterhood: A Novel   \n",
            "3  0786868716                 The Five People You Meet in Heaven   \n",
            "4  006101351X  The Perfect Storm : A True Story of Men Agains...   \n",
            "\n",
            "        Book-Author  ALS_Rating  Similarity  Final_Score  Source_User  \\\n",
            "0  MICHAEL CRICHTON    0.000000    0.746775     0.298710         4929   \n",
            "1  MICHAEL CRICHTON    0.000000    0.655019     0.262008         4929   \n",
            "2     Rebecca Wells    0.884480    0.384007     0.684291        14124   \n",
            "3       Mitch Albom    0.867778    0.385487     0.674861        14124   \n",
            "4  Sebastian Junger    0.779992    0.385014     0.622001        14124   \n",
            "\n",
            "  Source_Book  \n",
            "0  0842329129  \n",
            "1  0345380193  \n",
            "2  0140440488  \n",
            "3  0140440488  \n",
            "4  0140440488  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Get 5 random users and books for testing\n",
        "random_users = ratings_spark_df.select(\"User-ID\").distinct().limit(5)\n",
        "random_books = ratings_spark_df.select(\"ISBN\").distinct().limit(5)\n",
        "\n",
        "# Collect the users and books as lists\n",
        "user_list = [row[\"User-ID\"] for row in random_users.collect()]\n",
        "book_list = [row[\"ISBN\"] for row in random_books.collect()]\n",
        "\n",
        "print(\"Generating recommendations for 50 users and 50 books...\")\n",
        "\n",
        "# Initialize empty lists to store results\n",
        "all_recommendations = []\n",
        "\n",
        "# Generate recommendations for each user-book pair\n",
        "for user in user_list:\n",
        "    for book in book_list:\n",
        "        try:\n",
        "            # Get recommendations\n",
        "            recs = get_reranked_recommendations(user, book, num_recommendations=5)\n",
        "            if recs:\n",
        "                # Convert to pandas and add source user/book info\n",
        "                recs_pd = recs.toPandas()\n",
        "                recs_pd['Source_User'] = user\n",
        "                recs_pd['Source_Book'] = book\n",
        "                all_recommendations.append(recs_pd)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting recommendations for user {user}, book {book}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\nFinished generating recommendations\")\n",
        "\n",
        "# Combine all results\n",
        "if all_recommendations:\n",
        "    all_recs_df = pd.concat(all_recommendations, ignore_index=True)\n",
        "    print(\"\\nSample of recommendations:\")\n",
        "    print(all_recs_df)\n",
        "else:\n",
        "    print(\"No recommendations were generated successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57j7NVeHBkAa",
        "outputId": "f599b3e0-d5a0-4e57-9c17-aa416946f775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "📚 PERSONALIZED BOOK RECOMMENDATIONS 📚\n",
            "================================================================================\n",
            "Total recommendations generated: 15\n",
            "Unique users: 2\n",
            "Unique source books: 5\n",
            "--------------------------------------------------------------------------------\n",
            "Sample of recommendations:\n",
            "--------------------------------------------------------------------------------\n",
            "      ISBN                                              Book-Title        Book-Author  ALS_Rating  Similarity  Final_Score  Source_User Source_Book\n",
            "0345353145                                                  Sphere   MICHAEL CRICHTON    0.000000    0.746775     0.298710         4929  0842329129\n",
            "0345353145                                                  Sphere   MICHAEL CRICHTON    0.000000    0.655019     0.262008         4929  0345380193\n",
            "0060928336         Divine Secrets of the Ya-Ya Sisterhood: A Novel      Rebecca Wells    0.884480    0.384007     0.684291        14124  0140440488\n",
            "0786868716                      The Five People You Meet in Heaven        Mitch Albom    0.867778    0.385487     0.674861        14124  0140440488\n",
            "006101351X The Perfect Storm : A True Story of Men Against the Sea   Sebastian Junger    0.779992    0.385014     0.622001        14124  0140440488\n",
            "0060175400                                    The Poisonwood Bible Barbara Kingsolver    0.760413    0.407363     0.619193        14124  0140440488\n",
            "0316776963                                  Me Talk Pretty One Day      David Sedaris    0.742212    0.430032     0.617340        14124  0140440488\n",
            "0786868716                      The Five People You Meet in Heaven        Mitch Albom    0.867778    0.341684     0.657340        14124  0767905385\n",
            "0060928336         Divine Secrets of the Ya-Ya Sisterhood: A Novel      Rebecca Wells    0.884480    0.503185     0.731962        14124  0670865796\n",
            "0786868716                      The Five People You Meet in Heaven        Mitch Albom    0.867778    0.528229     0.731958        14124  0670865796\n",
            "0060175400                                    The Poisonwood Bible Barbara Kingsolver    0.760413    0.559598     0.680087        14124  0670865796\n",
            "006101351X The Perfect Storm : A True Story of Men Against the Sea   Sebastian Junger    0.779992    0.528898     0.679554        14124  0670865796\n",
            "0316776963                                  Me Talk Pretty One Day      David Sedaris    0.742212    0.573652     0.674788        14124  0670865796\n",
            "0060928336         Divine Secrets of the Ya-Ya Sisterhood: A Novel      Rebecca Wells    0.884480    0.457738     0.713783        14124  0842329129\n",
            "0060928336         Divine Secrets of the Ya-Ya Sisterhood: A Novel      Rebecca Wells    0.884480    0.450657     0.710950        14124  0345380193\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/27 16:00:07 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 731267 ms exceeds timeout 600000 ms\n",
            "25/05/27 16:00:07 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "25/05/27 16:32:08 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 16:32:08 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 17:11:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 17:11:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 17:48:03 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 17:48:03 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 18:40:12 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 18:40:12 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 19:10:42 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 19:10:42 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 20:24:24 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 20:24:24 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 21:09:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 21:09:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 21:51:45 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 21:51:45 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 22:24:53 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 22:24:53 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 22:52:38 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 22:52:38 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:03:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:03:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:04:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:04:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:05:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:05:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:06:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:06:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:07:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:07:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:08:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:08:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:09:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:09:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:10:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:10:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:11:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:11:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:12:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:12:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:13:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:13:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:14:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:14:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:15:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:15:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:16:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:16:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:17:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:17:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:18:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:18:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:19:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:19:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:20:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:20:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:21:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:21:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:22:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:22:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:23:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:23:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:24:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:24:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:25:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:25:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:26:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:26:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:27:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:27:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:28:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:28:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:29:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:29:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:30:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:30:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:31:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:31:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:32:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:32:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:33:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:33:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:34:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:34:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:35:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:35:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:36:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:36:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:37:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:37:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:38:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:38:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:39:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:39:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:40:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:40:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:41:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:41:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:42:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:42:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:43:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:43:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:44:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:44:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:45:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:45:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:46:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:46:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:47:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:47:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "25/05/27 23:48:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:48:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:49:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:49:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:50:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:50:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:51:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:51:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:52:30 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:52:30 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@localhost:55600\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "25/05/27 23:52:30 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
          ]
        }
      ],
      "source": [
        "if all_recommendations:\n",
        "    all_recs_df = pd.concat(all_recommendations, ignore_index=True)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📚 PERSONALIZED BOOK RECOMMENDATIONS 📚\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total recommendations generated: {len(all_recs_df)}\")\n",
        "    print(f\"Unique users: {all_recs_df['Source_User'].nunique()}\")\n",
        "    print(f\"Unique source books: {all_recs_df['Source_Book'].nunique()}\")\n",
        "    print(\"-\"*80)\n",
        "    print(\"Sample of recommendations:\")\n",
        "    print(\"-\"*80)\n",
        "    print(all_recs_df.to_string(index=False, max_rows=20))\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No recommendations were generated successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m8ccNmnBkAa"
      },
      "outputs": [],
      "source": [
        "print(all_recs_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}